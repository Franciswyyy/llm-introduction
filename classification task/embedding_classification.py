#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åµŒå…¥æ¨¡å‹åˆ†ç±»ä»»åŠ¡
ä½¿ç”¨Sentence Transformerå°†æ–‡æœ¬è½¬æ¢ä¸ºåµŒå…¥å‘é‡ï¼Œç„¶åè®­ç»ƒåˆ†ç±»å™¨è¿›è¡Œæƒ…æ„Ÿåˆ†æ

åŒ…å«ä¸¤ç§åˆ†ç±»æ–¹æ³•ï¼š
1. é€»è¾‘å›å½’åˆ†ç±»å™¨
2. åŸºäºä½™å¼¦ç›¸ä¼¼åº¦çš„åˆ†ç±»
"""

import sys
import os
import numpy as np
import pandas as pd
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥utilsæ¨¡å—
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.append(str(PROJECT_ROOT))

# å¯¼å…¥å¿…è¦çš„åº“
from sentence_transformers import SentenceTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns

# å¯¼å…¥æ•°æ®ç®¡ç†å·¥å…·
from utils import get_dataset

def load_data():
    """
    åŠ è½½Rotten Tomatoesæ•°æ®é›†
    
    Returns:
        dict: åŒ…å«trainã€validationã€teståˆ†å‰²çš„æ•°æ®é›†
    """
    print("ğŸ”„ æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    data = get_dataset()
    
    if data is None:
        raise Exception("æ•°æ®é›†åŠ è½½å¤±è´¥")
    
    print("âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ")
    print(f"è®­ç»ƒé›†å¤§å°: {len(data['train'])}")
    print(f"éªŒè¯é›†å¤§å°: {len(data['validation'])}")  
    print(f"æµ‹è¯•é›†å¤§å°: {len(data['test'])}")
    
    return data

def load_embedding_model():
    """
    åŠ è½½é¢„è®­ç»ƒçš„Sentence Transformeræ¨¡å‹
    
    Returns:
        SentenceTransformer: å·²åŠ è½½çš„æ¨¡å‹å®ä¾‹
    """
    print("ğŸ¤– æ­£åœ¨åŠ è½½Sentence Transformeræ¨¡å‹...")
    print("æ¨¡å‹: sentence-transformers/all-mpnet-base-v2")
    
    # åŠ è½½é¢„è®­ç»ƒçš„å¤šè¯­è¨€å¥å­åµŒå…¥æ¨¡å‹
    # all-mpnet-base-v2 æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„å¥å­åµŒå…¥æ¨¡å‹
    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')
    
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    return model

def generate_embeddings(model, data):
    """
    å°†æ–‡æœ¬è½¬æ¢ä¸ºåµŒå…¥å‘é‡
    
    Args:
        model: Sentence Transformeræ¨¡å‹
        data: æ•°æ®é›†
        
    Returns:
        tuple: (è®­ç»ƒé›†åµŒå…¥, æµ‹è¯•é›†åµŒå…¥)
    """
    print("ğŸ”„ æ­£åœ¨ç”Ÿæˆæ–‡æœ¬åµŒå…¥...")
    
    # å°†è®­ç»ƒé›†æ–‡æœ¬è½¬æ¢ä¸ºåµŒå…¥å‘é‡
    print("  æ­£åœ¨å¤„ç†è®­ç»ƒé›†...")
    train_embeddings = model.encode(data["train"]["text"], show_progress_bar=True)
    
    # å°†æµ‹è¯•é›†æ–‡æœ¬è½¬æ¢ä¸ºåµŒå…¥å‘é‡  
    print("  æ­£åœ¨å¤„ç†æµ‹è¯•é›†...")
    test_embeddings = model.encode(data["test"]["text"], show_progress_bar=True)
    
    print(f"âœ… åµŒå…¥ç”Ÿæˆå®Œæˆ")
    print(f"è®­ç»ƒé›†åµŒå…¥å½¢çŠ¶: {train_embeddings.shape}")
    print(f"æµ‹è¯•é›†åµŒå…¥å½¢çŠ¶: {test_embeddings.shape}")
    
    return train_embeddings, test_embeddings

def train_logistic_regression(train_embeddings, train_labels):
    """
    è®­ç»ƒé€»è¾‘å›å½’åˆ†ç±»å™¨
    
    Args:
        train_embeddings: è®­ç»ƒé›†åµŒå…¥å‘é‡
        train_labels: è®­ç»ƒé›†æ ‡ç­¾
        
    Returns:
        LogisticRegression: è®­ç»ƒå¥½çš„åˆ†ç±»å™¨
    """
    print("ğŸ¯ æ­£åœ¨è®­ç»ƒé€»è¾‘å›å½’åˆ†ç±»å™¨...")
    
    # åˆ›å»ºé€»è¾‘å›å½’åˆ†ç±»å™¨
    # random_state=42 ç¡®ä¿ç»“æœå¯é‡ç°
    clf = LogisticRegression(random_state=42, max_iter=1000)
    
    # åœ¨è®­ç»ƒé›†åµŒå…¥ä¸Šè®­ç»ƒåˆ†ç±»å™¨
    clf.fit(train_embeddings, train_labels)
    
    print("âœ… é€»è¾‘å›å½’è®­ç»ƒå®Œæˆ")
    return clf

def cosine_similarity_classification(train_embeddings, train_labels, test_embeddings):
    """
    åŸºäºä½™å¼¦ç›¸ä¼¼åº¦çš„åˆ†ç±»æ–¹æ³•
    
    è¯¥æ–¹æ³•è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¹³å‡åµŒå…¥å‘é‡ï¼Œç„¶åå°†æµ‹è¯•æ ·æœ¬ä¸æœ€ç›¸ä¼¼çš„ç±»åˆ«åŒ¹é…
    
    Args:
        train_embeddings: è®­ç»ƒé›†åµŒå…¥å‘é‡
        train_labels: è®­ç»ƒé›†æ ‡ç­¾
        test_embeddings: æµ‹è¯•é›†åµŒå…¥å‘é‡
        
    Returns:
        numpy.ndarray: é¢„æµ‹æ ‡ç­¾
    """
    print("ğŸ”„ æ­£åœ¨æ‰§è¡ŒåŸºäºä½™å¼¦ç›¸ä¼¼åº¦çš„åˆ†ç±»...")
    
    # å°†åµŒå…¥å’Œæ ‡ç­¾åˆå¹¶ä¸ºDataFrameä»¥ä¾¿åˆ†ç»„æ“ä½œ
    # å‡è®¾åµŒå…¥ç»´åº¦ä¸º768ï¼ˆall-mpnet-base-v2çš„è¾“å‡ºç»´åº¦ï¼‰
    embedding_dim = train_embeddings.shape[1]
    
    # åˆ›å»ºåŒ…å«åµŒå…¥å’Œæ ‡ç­¾çš„DataFrame
    df_data = np.hstack([train_embeddings, np.array(train_labels).reshape(-1, 1)])
    df = pd.DataFrame(df_data)
    
    # æŒ‰æ ‡ç­¾åˆ†ç»„ï¼Œè®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¹³å‡åµŒå…¥å‘é‡
    # æœ€åä¸€åˆ—æ˜¯æ ‡ç­¾ï¼Œå‰é¢çš„åˆ—æ˜¯åµŒå…¥ç‰¹å¾
    averaged_target_embeddings = df.groupby(embedding_dim).mean().iloc[:, :-1].values
    
    print(f"ç±»åˆ«å¹³å‡åµŒå…¥å½¢çŠ¶: {averaged_target_embeddings.shape}")
    
    # è®¡ç®—æµ‹è¯•åµŒå…¥ä¸æ¯ä¸ªç±»åˆ«å¹³å‡åµŒå…¥çš„ä½™å¼¦ç›¸ä¼¼åº¦
    sim_matrix = cosine_similarity(test_embeddings, averaged_target_embeddings)
    
    # é€‰æ‹©ç›¸ä¼¼åº¦æœ€é«˜çš„ç±»åˆ«ä½œä¸ºé¢„æµ‹ç»“æœ
    y_pred = np.argmax(sim_matrix, axis=1)
    
    print("âœ… ä½™å¼¦ç›¸ä¼¼åº¦åˆ†ç±»å®Œæˆ")
    return y_pred

def evaluate_performance(y_true, y_pred, method_name=""):
    """
    è¯„ä¼°åˆ†ç±»æ€§èƒ½
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾  
        method_name: æ–¹æ³•åç§°ï¼ˆç”¨äºè¾“å‡ºæ ‡è¯†ï¼‰
    """
    print(f"\nğŸ“Š {method_name}æ€§èƒ½è¯„ä¼°:")
    print("=" * 50)
    
    # æ‰“å°è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š
    report = classification_report(y_true, y_pred, 
                                 target_names=['è´Ÿé¢è¯„ä»·', 'æ­£é¢è¯„ä»·'],
                                 digits=4)
    print(report)
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    
    # å¯è§†åŒ–æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['è´Ÿé¢è¯„ä»·', 'æ­£é¢è¯„ä»·'],
                yticklabels=['è´Ÿé¢è¯„ä»·', 'æ­£é¢è¯„ä»·'])
    plt.title(f'{method_name}æ··æ·†çŸ©é˜µ')
    plt.ylabel('å®é™…æ ‡ç­¾')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    
    # ä¿å­˜æ··æ·†çŸ©é˜µå›¾åƒ
    output_path = Path(__file__).parent / f'{method_name}_confusion_matrix.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³: {output_path}")
    plt.show()

def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„åµŒå…¥åˆ†ç±»æµç¨‹
    """
    print("ğŸš€ å¼€å§‹æ‰§è¡ŒåµŒå…¥æ¨¡å‹åˆ†ç±»ä»»åŠ¡")
    print("=" * 60)
    
    try:
        # 1. åŠ è½½æ•°æ®é›†
        data = load_data()
        
        # 2. åŠ è½½åµŒå…¥æ¨¡å‹
        model = load_embedding_model()
        
        # 3. ç”ŸæˆåµŒå…¥å‘é‡
        train_embeddings, test_embeddings = generate_embeddings(model, data)
        
        # 4. æ–¹æ³•ä¸€ï¼šé€»è¾‘å›å½’åˆ†ç±»
        print("\n" + "="*60)
        print("æ–¹æ³•ä¸€ï¼šé€»è¾‘å›å½’åˆ†ç±»")
        print("="*60)
        
        clf = train_logistic_regression(train_embeddings, data["train"]["label"])
        lr_predictions = clf.predict(test_embeddings)
        evaluate_performance(data["test"]["label"], lr_predictions, "é€»è¾‘å›å½’")
        
        # 5. æ–¹æ³•äºŒï¼šä½™å¼¦ç›¸ä¼¼åº¦åˆ†ç±»  
        print("\n" + "="*60)
        print("æ–¹æ³•äºŒï¼šä½™å¼¦ç›¸ä¼¼åº¦åˆ†ç±»")
        print("="*60)
        
        cosine_predictions = cosine_similarity_classification(
            train_embeddings, data["train"]["label"], test_embeddings
        )
        evaluate_performance(data["test"]["label"], cosine_predictions, "ä½™å¼¦ç›¸ä¼¼åº¦")
        
        print("\nğŸ‰ åˆ†ç±»ä»»åŠ¡å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 