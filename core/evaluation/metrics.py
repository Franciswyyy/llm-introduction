#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Evaluation Metrics - è¯„ä¼°æŒ‡æ ‡
æä¾›å„ç§æ¨¡å‹è¯„ä¼°åŠŸèƒ½
"""

from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
import numpy as np
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_auc_score
)


class BaseEvaluator(ABC):
    """è¯„ä¼°å™¨åŸºç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
    
    @abstractmethod
    def evaluate(self, y_true: np.ndarray, y_pred: np.ndarray, 
                **kwargs) -> Dict[str, Any]:
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½
        
        Args:
            y_true: çœŸå®æ ‡ç­¾
            y_pred: é¢„æµ‹æ ‡ç­¾
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        pass


class ClassificationEvaluator(BaseEvaluator):
    """åˆ†ç±»ä»»åŠ¡è¯„ä¼°å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.metrics = config.get('metrics', ['accuracy', 'precision', 'recall', 'f1'])
        self.average = config.get('average', 'weighted')
        self.class_names = config.get('class_names', None)
    
    def evaluate(self, y_true: np.ndarray, y_pred: np.ndarray, 
                y_proba: Optional[np.ndarray] = None,
                class_names: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        è¯„ä¼°åˆ†ç±»æ¨¡å‹æ€§èƒ½
        
        Args:
            y_true: çœŸå®æ ‡ç­¾
            y_pred: é¢„æµ‹æ ‡ç­¾
            y_proba: é¢„æµ‹æ¦‚ç‡ï¼ˆå¯é€‰ï¼‰
            class_names: ç±»åˆ«åç§°ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        results = {}
        
        # ä½¿ç”¨ä¼ å…¥çš„ç±»åˆ«åç§°æˆ–é…ç½®ä¸­çš„ç±»åˆ«åç§°
        if class_names is not None:
            self.class_names = class_names
        
        # åŸºç¡€æŒ‡æ ‡
        if 'accuracy' in self.metrics:
            results['accuracy'] = accuracy_score(y_true, y_pred)
        
        if 'precision' in self.metrics:
            results['precision'] = precision_score(y_true, y_pred, average=self.average, zero_division=0)
        
        if 'recall' in self.metrics:
            results['recall'] = recall_score(y_true, y_pred, average=self.average, zero_division=0)
        
        if 'f1' in self.metrics:
            results['f1'] = f1_score(y_true, y_pred, average=self.average, zero_division=0)
        
        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        if 'classification_report' in self.metrics:
            results['classification_report'] = classification_report(
                y_true, y_pred, 
                target_names=self.class_names,
                output_dict=True,
                zero_division=0
            )
        
        # æ··æ·†çŸ©é˜µ
        if 'confusion_matrix' in self.metrics:
            results['confusion_matrix'] = confusion_matrix(y_true, y_pred)
        
        # ROC AUC (å¦‚æœæœ‰æ¦‚ç‡é¢„æµ‹)
        if y_proba is not None and 'roc_auc' in self.metrics:
            try:
                if len(np.unique(y_true)) == 2:  # äºŒåˆ†ç±»
                    results['roc_auc'] = roc_auc_score(y_true, y_proba[:, 1])
                else:  # å¤šåˆ†ç±»
                    results['roc_auc'] = roc_auc_score(y_true, y_proba, multi_class='ovr', average=self.average)
            except Exception as e:
                results['roc_auc_error'] = str(e)
        
        # æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
        if 'per_class_metrics' in self.metrics:
            results['per_class_metrics'] = self._compute_per_class_metrics(y_true, y_pred)
        
        return results
    
    def _compute_per_class_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, Any]:
        """
        è®¡ç®—æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
        
        Args:
            y_true: çœŸå®æ ‡ç­¾
            y_pred: é¢„æµ‹æ ‡ç­¾
            
        Returns:
            æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡å­—å…¸
        """
        unique_labels = np.unique(np.concatenate([y_true, y_pred]))
        per_class_results = {}
        
        for label in unique_labels:
            class_name = self.class_names[label] if self.class_names and label < len(self.class_names) else f"Class_{label}"
            
            # è®¡ç®—äºŒåˆ†ç±»æŒ‡æ ‡
            y_true_binary = (y_true == label).astype(int)
            y_pred_binary = (y_pred == label).astype(int)
            
            per_class_results[class_name] = {
                'precision': precision_score(y_true_binary, y_pred_binary, zero_division=0),
                'recall': recall_score(y_true_binary, y_pred_binary, zero_division=0),
                'f1': f1_score(y_true_binary, y_pred_binary, zero_division=0),
                'support': np.sum(y_true == label)
            }
        
        return per_class_results
    
    def compare_models(self, results_list: List[Dict[str, Any]], 
                      model_names: List[str]) -> Dict[str, Any]:
        """
        æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„æ€§èƒ½
        
        Args:
            results_list: å¤šä¸ªæ¨¡å‹çš„è¯„ä¼°ç»“æœåˆ—è¡¨
            model_names: æ¨¡å‹åç§°åˆ—è¡¨
            
        Returns:
            æ¨¡å‹æ¯”è¾ƒç»“æœ
        """
        comparison = {}
        
        # æå–å…¬å…±æŒ‡æ ‡
        common_metrics = set(results_list[0].keys())
        for results in results_list[1:]:
            common_metrics &= set(results.keys())
        
        # æ’é™¤éæ•°å€¼æŒ‡æ ‡
        numeric_metrics = []
        for metric in common_metrics:
            if isinstance(results_list[0][metric], (int, float)):
                numeric_metrics.append(metric)
        
        # æ„å»ºæ¯”è¾ƒè¡¨
        for metric in numeric_metrics:
            comparison[metric] = {}
            values = [results[metric] for results in results_list]
            
            for i, (name, value) in enumerate(zip(model_names, values)):
                comparison[metric][name] = value
            
            # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
            if metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:
                best_idx = np.argmax(values)
            else:
                best_idx = np.argmin(values)  # å¯¹äºæŸå¤±å‡½æ•°ç­‰
            
            comparison[metric]['best_model'] = model_names[best_idx]
            comparison[metric]['best_value'] = values[best_idx]
        
        return comparison
    
    def generate_summary(self, results: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆè¯„ä¼°ç»“æœæ‘˜è¦
        
        Args:
            results: è¯„ä¼°ç»“æœå­—å…¸
            
        Returns:
            æ ¼å¼åŒ–çš„æ‘˜è¦å­—ç¬¦ä¸²
        """
        summary_lines = ["ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœæ‘˜è¦", "=" * 50]
        
        # ä¸»è¦æŒ‡æ ‡
        main_metrics = ['accuracy', 'precision', 'recall', 'f1']
        for metric in main_metrics:
            if metric in results:
                summary_lines.append(f"{metric.upper()}: {results[metric]:.4f}")
        
        # ROC AUC
        if 'roc_auc' in results:
            summary_lines.append(f"ROC AUC: {results['roc_auc']:.4f}")
        
        # æ··æ·†çŸ©é˜µå½¢çŠ¶
        if 'confusion_matrix' in results:
            cm = results['confusion_matrix']
            summary_lines.append(f"æ··æ·†çŸ©é˜µå½¢çŠ¶: {cm.shape}")
            summary_lines.append(f"æ€»é¢„æµ‹æ ·æœ¬æ•°: {cm.sum()}")
        
        return "\n".join(summary_lines) 