#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Core æ¨¡å—ä½¿ç”¨ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨å·²å°è£…çš„5ä¸ªæ ¸å¿ƒæ¨¡å—è¿›è¡Œæ–‡æœ¬åˆ†ç±»ä»»åŠ¡
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.append(str(PROJECT_ROOT))

# å¯¼å…¥Coreæ¨¡å—
from core.data.loaders import HuggingFaceLoader, CSVLoader
from core.embeddings.sentence_transformer import SentenceTransformerEmbedding
from core.classifiers.supervised import LogisticRegressionClassifier
from core.classifiers.similarity import SimilarityClassifier
from core.evaluation.metrics import ClassificationEvaluator
from core.pipeline.text_classification import TextClassificationPipeline

print("ğŸ—ï¸ Coreæ¨¡å—ä½¿ç”¨æ¼”ç¤º")
print("=" * 60)

def demo_individual_modules():
    """æ¼”ç¤ºå„ä¸ªæ¨¡å—çš„ç‹¬ç«‹ä½¿ç”¨"""
    print("\nğŸ“‹ æ–¹å¼ä¸€ï¼šåˆ†æ­¥ä½¿ç”¨å„ä¸ªæ¨¡å—")
    print("-" * 40)
    
    # é˜¶æ®µ1: æ•°æ®åŠ è½½
    print("ğŸ”¸ é˜¶æ®µ1: æ•°æ®åŠ è½½")
    data_config = {
        "dataset_name": "rotten_tomatoes",
        "cache_dir": "./resources/datasets"
    }
    data_loader = HuggingFaceLoader(data_config)
    dataset = data_loader.load()
    train_data = dataset['train']
    test_data = dataset['test']
    print(f"   è®­ç»ƒæ•°æ®: {len(train_data)} æ¡")
    print(f"   æµ‹è¯•æ•°æ®: {len(test_data)} æ¡")
    
    # é˜¶æ®µ2: åµŒå…¥ç”Ÿæˆ
    print("\nğŸ”¸ é˜¶æ®µ2: åµŒå…¥ç”Ÿæˆ")
    embedding_config = {
        "model_name": "sentence-transformers/all-mpnet-base-v2",
        "batch_size": 16,
        "normalize_embeddings": True
    }
    embedding_model = SentenceTransformerEmbedding(embedding_config)
    
    # å–æ ·æœ¬æ•°æ®è¿›è¡Œæ¼”ç¤ºï¼ˆé¿å…è®¡ç®—æ—¶é—´è¿‡é•¿ï¼‰
    sample_train = train_data[:100]
    sample_test = test_data[:50]
    
    train_embeddings = embedding_model.encode([item['text'] for item in sample_train])
    test_embeddings = embedding_model.encode([item['text'] for item in sample_test])
    print(f"   è®­ç»ƒåµŒå…¥: {train_embeddings.shape}")
    print(f"   æµ‹è¯•åµŒå…¥: {test_embeddings.shape}")
    
    # é˜¶æ®µ3: è®­ç»ƒåˆ†ç±»å™¨
    print("\nğŸ”¸ é˜¶æ®µ3: è®­ç»ƒåˆ†ç±»å™¨")
    classifier_config = {
        "random_state": 42,
        "max_iter": 1000,
        "C": 1.0
    }
    classifier = LogisticRegressionClassifier(classifier_config)
    
    train_labels = [item['label'] for item in sample_train]
    test_labels = [item['label'] for item in sample_test]
    
    classifier.train(train_embeddings, train_labels)
    print("   âœ… åˆ†ç±»å™¨è®­ç»ƒå®Œæˆ")
    
    # é˜¶æ®µ4: é¢„æµ‹
    print("\nğŸ”¸ é˜¶æ®µ4: é¢„æµ‹")
    predictions = classifier.predict(test_embeddings)
    probabilities = classifier.predict_proba(test_embeddings)
    print(f"   é¢„æµ‹ç»“æœ: {len(predictions)} ä¸ª")
    print(f"   é¢„æµ‹æ¦‚ç‡: {probabilities.shape}")
    
    # é˜¶æ®µ5: è¯„ä¼°
    print("\nğŸ”¸ é˜¶æ®µ5: è¯„ä¼°")
    eval_config = {
        "target_names": ["è´Ÿé¢", "æ­£é¢"]
    }
    evaluator = ClassificationEvaluator(eval_config)
    metrics = evaluator.evaluate(test_labels, predictions)
    print(f"   å‡†ç¡®ç‡: {metrics['accuracy']:.3f}")
    print(f"   F1åˆ†æ•°: {metrics['f1_macro']:.3f}")

def demo_pipeline_usage():
    """æ¼”ç¤ºä½¿ç”¨Pipelineè¿›è¡Œç«¯åˆ°ç«¯å¤„ç†"""
    print("\n\nğŸ“‹ æ–¹å¼äºŒï¼šä½¿ç”¨Pipelineç«¯åˆ°ç«¯å¤„ç†")
    print("-" * 40)
    
    # ç›´æ¥ä½¿ç”¨é…ç½®æ–‡ä»¶
    config_path = PROJECT_ROOT / "utils" / "sentiment_analysis.yaml"
    
    print("ğŸ”¸ ä»é…ç½®æ–‡ä»¶åˆ›å»ºPipeline")
    pipeline = TextClassificationPipeline.from_config_file(str(config_path))
    print("   âœ… Pipelineåˆ›å»ºå®Œæˆ")
    
    print("\nğŸ”¸ æ‰§è¡Œå®Œæ•´æµç¨‹")
    try:
        results = pipeline.run()
        print("   âœ… Pipelineæ‰§è¡Œå®Œæˆ")
        print(f"   æœ€ä½³æ¨¡å‹: {results['best_model']}")
        print(f"   æœ€ä½³F1åˆ†æ•°: {results['best_f1']:.3f}")
        
        # æ˜¾ç¤ºå„ä¸ªåˆ†ç±»å™¨çš„ç»“æœ
        print("\n   ğŸ“Š å„åˆ†ç±»å™¨æ€§èƒ½:")
        for model_name, metrics in results['results'].items():
            print(f"      {model_name}: F1={metrics['f1_macro']:.3f}")
            
    except Exception as e:
        print(f"   âš ï¸ Pipelineæ‰§è¡Œå‡ºé”™: {e}")
        print("   è¿™å¯èƒ½æ˜¯å› ä¸ºè®¡ç®—èµ„æºé™åˆ¶ï¼Œåœ¨å®é™…ä½¿ç”¨ä¸­é€šå¸¸èƒ½æ­£å¸¸è¿è¡Œ")

def demo_similarity_classifier():
    """æ¼”ç¤ºç›¸ä¼¼åº¦åˆ†ç±»å™¨çš„ä½¿ç”¨"""
    print("\n\nğŸ“‹ æ–¹å¼ä¸‰ï¼šä½¿ç”¨ç›¸ä¼¼åº¦åˆ†ç±»å™¨ï¼ˆæ— éœ€è®­ç»ƒï¼‰")
    print("-" * 40)
    
    # å‡†å¤‡ç¤ºä¾‹æ•°æ®
    texts = [
        "è¿™éƒ¨ç”µå½±çœŸçš„å¾ˆæ£’ï¼Œæˆ‘éå¸¸å–œæ¬¢ï¼",
        "å¤ªç³Ÿç³•äº†ï¼Œå®Œå…¨æµªè´¹æ—¶é—´ã€‚",
        "è¿˜ä¸é”™ï¼Œå€¼å¾—ä¸€çœ‹ã€‚",
        "ç»å¯¹æ˜¯åƒåœ¾ç”µå½±ï¼Œä¸æ¨èã€‚"
    ]
    
    # ç±»åˆ«åŸå‹æ–‡æœ¬
    prototype_texts = {
        0: ["è¿™å¾ˆç³Ÿç³•", "æˆ‘ä¸å–œæ¬¢", "å¤ªå·®äº†"],  # è´Ÿé¢
        1: ["è¿™å¾ˆæ£’", "æˆ‘å–œæ¬¢", "å¤ªå¥½äº†"]      # æ­£é¢
    }
    
    print("ğŸ”¸ åˆ›å»ºç›¸ä¼¼åº¦åˆ†ç±»å™¨")
    similarity_config = {
        "metric": "cosine",
        "embedding_model": "sentence-transformers/all-mpnet-base-v2"
    }
    similarity_classifier = SimilarityClassifier(similarity_config)
    
    print("ğŸ”¸ è®¾ç½®ç±»åˆ«åŸå‹")
    similarity_classifier.set_prototypes(prototype_texts)
    
    print("ğŸ”¸ è¿›è¡Œåˆ†ç±»é¢„æµ‹")
    for i, text in enumerate(texts):
        prediction = similarity_classifier.predict([text])[0]
        confidence = similarity_classifier.predict_proba([text])[0]
        
        label = "æ­£é¢" if prediction == 1 else "è´Ÿé¢"
        conf_score = max(confidence)
        
        print(f"   æ–‡æœ¬{i+1}: {text[:20]}...")
        print(f"   é¢„æµ‹: {label} (ç½®ä¿¡åº¦: {conf_score:.3f})")

def demo_custom_data():
    """æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®"""
    print("\n\nğŸ“‹ æ–¹å¼å››ï¼šä½¿ç”¨è‡ªå®šä¹‰æ•°æ®")
    print("-" * 40)
    
    # åˆ›å»ºè‡ªå®šä¹‰æ•°æ®
    import pandas as pd
    custom_data = pd.DataFrame({
        'text': [
            "è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½ï¼Œæ¨èè´­ä¹°",
            "æœåŠ¡æ€åº¦å·®ï¼Œä¸ä¼šå†æ¥äº†",
            "ä»·æ ¼åˆç†ï¼Œæ€§ä»·æ¯”ä¸é”™",
            "å®Œå…¨ä¸å€¼è¿™ä¸ªä»·é’±",
            "å¿«é€’å¾ˆå¿«ï¼ŒåŒ…è£…ä¹Ÿå¾ˆå¥½"
        ],
        'label': [1, 0, 1, 0, 1]  # 1=æ­£é¢, 0=è´Ÿé¢
    })
    
    # ä¿å­˜ä¸ºCSVæ–‡ä»¶
    csv_path = PROJECT_ROOT / "resources" / "custom_data.csv"
    csv_path.parent.mkdir(exist_ok=True)
    custom_data.to_csv(csv_path, index=False)
    print(f"ğŸ”¸ åˆ›å»ºè‡ªå®šä¹‰æ•°æ®æ–‡ä»¶: {csv_path}")
    
    # ä½¿ç”¨CSVåŠ è½½å™¨
    csv_config = {
        "file_path": str(csv_path),
        "text_column": "text",
        "label_column": "label"
    }
    csv_loader = CSVLoader(csv_config)
    dataset = csv_loader.load()
    data = dataset['train']  # CSVåŠ è½½å™¨è¿”å›çš„æ ¼å¼
    
    print(f"   åŠ è½½æ•°æ®: {len(data)} æ¡")
    for i, item in enumerate(data[:3]):
        print(f"   æ ·æœ¬{i+1}: {item['text'][:30]}... (æ ‡ç­¾: {item['label']})")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ Coreæ¨¡å—æ¶æ„è¯´æ˜:")
    print("   1ï¸âƒ£ core.data      - æ•°æ®åŠ è½½å’Œé¢„å¤„ç†")
    print("   2ï¸âƒ£ core.embeddings - æ–‡æœ¬åµŒå…¥ç”Ÿæˆ") 
    print("   3ï¸âƒ£ core.classifiers - åˆ†ç±»å™¨è®­ç»ƒå’Œé¢„æµ‹")
    print("   4ï¸âƒ£ core.evaluation - æ€§èƒ½è¯„ä¼°å’Œå¯è§†åŒ–")
    print("   5ï¸âƒ£ core.pipeline   - ç«¯åˆ°ç«¯æµæ°´çº¿")
    
    try:
        # æ¼”ç¤ºå„ä¸ªä½¿ç”¨æ–¹å¼
        demo_individual_modules()
        demo_similarity_classifier()
        demo_custom_data()
        demo_pipeline_usage()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ Coreæ¨¡å—æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print("   â€¢ å¿«é€ŸåŸå‹: ä½¿ç”¨Pipeline")
        print("   â€¢ çµæ´»å®šåˆ¶: åˆ†æ­¥ä½¿ç”¨å„æ¨¡å—")
        print("   â€¢ æ— ç›‘ç£åˆ†ç±»: ä½¿ç”¨ç›¸ä¼¼åº¦åˆ†ç±»å™¨")
        print("   â€¢ ç”Ÿäº§ç¯å¢ƒ: ç»“åˆé…ç½®æ–‡ä»¶ä½¿ç”¨")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¿™å¯èƒ½æ˜¯ç”±äºè®¡ç®—èµ„æºé™åˆ¶ï¼Œæ ¸å¿ƒåŠŸèƒ½ä»ç„¶å¯ç”¨")

if __name__ == "__main__":
    main() 