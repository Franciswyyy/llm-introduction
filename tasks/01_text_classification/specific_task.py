#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é…ç½®åŒ–é¢„è®­ç»ƒæ¨¡å‹è¯„ä¼°è„šæœ¬
æ”¯æŒé€šè¿‡é…ç½®æ–‡ä»¶æŒ‡å®šæ•°æ®é›†å’Œæ¨¡å‹å‚æ•°
åŸºäºç»Ÿä¸€é…ç½®ç®¡ç†ç³»ç»Ÿï¼Œæä¾›çµæ´»çš„å®éªŒé…ç½®
"""

import sys
from pathlib import Path
import argparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.append(str(PROJECT_ROOT))

# å¯¼å…¥é…ç½®ç®¡ç†å’Œæ•°æ®åŠ è½½
from utils import config, get_config, load_task_config
from core.data.loaders import HuggingFaceLoader
from transformers import pipeline
import numpy as np
from tqdm import tqdm
from transformers.pipelines.pt_utils import KeyDataset
from sklearn.metrics import classification_report


def load_data_with_config(data_config):
    """æ ¹æ®é…ç½®åŠ è½½æ•°æ®é›†"""
    print(f"ğŸ“Š åŠ è½½æ•°æ®é›†é…ç½®:")
    print(f"   æ•°æ®é›†: {data_config.get('dataset_name', 'rotten_tomatoes')}")
    print(f"   åŠ è½½å™¨ç±»å‹: {data_config.get('loader_type', 'HuggingFaceLoader')}")
    
    loader_type = data_config.get('loader_type', 'HuggingFaceLoader')
    
    if loader_type == 'HuggingFaceLoader':
        # ä½¿ç”¨ç»“æ„åŒ–çš„æ•°æ®åŠ è½½å™¨
        loader = HuggingFaceLoader(data_config)
        dataset = loader.load()
        return dataset
    else:
        # å›é€€åˆ°åŸæœ‰æ–¹å¼
        from utils import get_dataset
        return get_dataset()


def load_model_with_config(model_config):
    """æ ¹æ®é…ç½®åŠ è½½æ¨¡å‹"""
    model_path = model_config.get('sentiment_model', 'cardiffnlp/twitter-roberta-base-sentiment-latest')
    device = model_config.get('device', 'auto')
    
    print(f"ğŸ¤– åŠ è½½æ¨¡å‹é…ç½®:")
    print(f"   æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"   è®¾å¤‡: {device}")
    
    # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
    if device == 'auto':
        from utils import get_device
        device = get_device()
        print(f"   è‡ªåŠ¨é€‰æ‹©è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹ç®¡é“
    pipe = pipeline(
        model=model_path,
        tokenizer=model_path,
        return_all_scores=True,
        device=device if device != 'cpu' else -1  # transformersæœŸæœ›-1è¡¨ç¤ºCPU
    )
    
    return pipe


def run_evaluation(dataset, model_pipe, task_config):
    """è¿è¡Œæ¨¡å‹è¯„ä¼°"""
    print(f"ğŸ”„ å¼€å§‹è¯„ä¼°...")
    
    # è·å–æµ‹è¯•æ•°æ®
    test_data = dataset["test"]
    print(f"   æµ‹è¯•æ ·æœ¬æ•°: {len(test_data):,}")
    
    # æ‰¹é‡é¢„æµ‹
    y_pred = []
    for output in tqdm(model_pipe(KeyDataset(test_data, "text")), 
                      total=len(test_data), 
                      desc="é¢„æµ‹è¿›åº¦"):
        # è§£ææƒ…æ„Ÿåˆ†æç»“æœ (negative, neutral, positive)
        negative_score = output[0]["score"]  # NEGATIVE
        positive_score = output[2]["score"]   # POSITIVE
        # äºŒåˆ†ç±»ï¼šé€‰æ‹©negativeå’Œpositiveä¸­åˆ†æ•°æ›´é«˜çš„
        assignment = np.argmax([negative_score, positive_score])
        y_pred.append(assignment)
    
    # è·å–çœŸå®æ ‡ç­¾
    y_true = test_data["label"]
    
    # è¯„ä¼°æ€§èƒ½
    evaluate_performance(y_true, y_pred, task_config)


def evaluate_performance(y_true, y_pred, task_config):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print(f"\nğŸ“ˆ è¯„ä¼°ç»“æœ:")
    
    # è·å–æ ‡ç­¾åç§°
    label_names = ["Negative Review", "Positive Review"]
    
    # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
    performance = classification_report(
        y_true, y_pred,
        target_names=label_names,
        digits=4
    )
    
    print(performance)
    
    # ä¿å­˜ç»“æœï¼ˆå¦‚æœé…ç½®è¦æ±‚ï¼‰
    if task_config.get('pipeline', {}).get('save_results', False):
        output_dir = config.get_path('RESULTS_DIR')
        output_dir.mkdir(exist_ok=True)
        
        result_file = output_dir / f"{task_config['task']['name']}_results.txt"
        with open(result_file, 'w', encoding='utf-8') as f:
            f.write(f"ä»»åŠ¡: {task_config['task']['name']}\n")
            f.write(f"æè¿°: {task_config['task']['description']}\n\n")
            f.write(performance)
        
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {result_file}")


def main(config_file: str = "config.yaml"):
    """ä¸»å‡½æ•°"""
    
    print(f"ğŸš€ å¯åŠ¨é…ç½®åŒ–æ–‡æœ¬åˆ†ç±»è¯„ä¼°")
    print(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT}")
    print(f"âš™ï¸  é…ç½®æ–‡ä»¶: {config_file}")
    
    try:
        # 1. åŠ è½½é…ç½®
        print(f"\nâš™ï¸  åŠ è½½é…ç½®...")
        task_config = load_task_config(config_file)
        
        data_config = task_config['data']
        model_config = task_config['models']
        
        # 2. åŠ è½½æ•°æ®é›†
        print(f"\nğŸ“Š åŠ è½½æ•°æ®é›†...")
        dataset = load_data_with_config(data_config)
        if dataset is None:
            raise Exception("æ•°æ®é›†åŠ è½½å¤±è´¥")
        
        # 3. åŠ è½½æ¨¡å‹
        print(f"\nğŸ¤– åŠ è½½æ¨¡å‹...")
        model_pipe = load_model_with_config(model_config)
        
        # 4. è¿è¡Œè¯„ä¼°
        print(f"\nğŸ”¬ è¿è¡Œè¯„ä¼°...")
        run_evaluation(dataset, model_pipe, task_config)
        
        print(f"\nâœ… è¯„ä¼°å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # æ”¯æŒå‘½ä»¤è¡Œå‚æ•°æŒ‡å®šé…ç½®æ–‡ä»¶
    parser = argparse.ArgumentParser(description='é…ç½®åŒ–æ–‡æœ¬åˆ†ç±»è¯„ä¼°')
    parser.add_argument('--config', '-c', 
                       default='config.yaml',
                       help='é…ç½®æ–‡ä»¶åç§° (é»˜è®¤: config.yaml)')
    
    args = parser.parse_args()
    main(args.config) 