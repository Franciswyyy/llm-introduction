#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†ææ¨¡å‹è®­ç»ƒè„šæœ¬
ä½¿ç”¨ twitter-roberta-base-sentiment-latest æ¨¡å‹å’Œ Rotten Tomatoes æ•°æ®é›†
æ‰€æœ‰æ–‡ä»¶æœ¬åœ°ç®¡ç†ï¼Œé’ˆå¯¹ Mac M2 ä¼˜åŒ–
"""

import os
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from datasets import load_dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    TrainingArguments, 
    Trainer,
    DataCollatorWithPadding
)
import warnings
warnings.filterwarnings('ignore')

# é¡¹ç›®æœ¬åœ°è·¯å¾„é…ç½®
PROJECT_ROOT = Path(__file__).parent
MODELS_DIR = PROJECT_ROOT / "models"
DATASETS_DIR = PROJECT_ROOT / "datasets"
PRETRAINED_MODEL_DIR = MODELS_DIR / "twitter-roberta-base-sentiment-latest"
TRAINED_MODEL_DIR = PROJECT_ROOT / "trained_model"

def setup_directories():
    """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
    directories = [MODELS_DIR, DATASETS_DIR, TRAINED_MODEL_DIR]
    for directory in directories:
        directory.mkdir(exist_ok=True, parents=True)

def get_device():
    """è·å–æœ€ä½³è®¾å¤‡é…ç½®ï¼ˆMac M2 MPSä¼˜å…ˆï¼‰"""
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("âœ… ä½¿ç”¨ Apple MPS åŠ é€Ÿ")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        print("âœ… ä½¿ç”¨ CUDA åŠ é€Ÿ")
    else:
        device = torch.device("cpu")
        print("âš ï¸  ä½¿ç”¨ CPU")
    return device

def download_and_save_model():
    """ä¸‹è½½å¹¶ä¿å­˜æ¨¡å‹åˆ°æœ¬åœ°"""
    model_name = "cardiffnlp/twitter-roberta-base-sentiment-latest"
    
    if PRETRAINED_MODEL_DIR.exists() and any(PRETRAINED_MODEL_DIR.iterdir()):
        print(f"âœ… æ¨¡å‹å·²å­˜åœ¨: {PRETRAINED_MODEL_DIR}")
        return str(PRETRAINED_MODEL_DIR)
    
    print(f"ğŸ“¥ ä¸‹è½½æ¨¡å‹: {model_name} (~500MB)")
    print("é¦–æ¬¡ä¸‹è½½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(model_name)
        
        PRETRAINED_MODEL_DIR.mkdir(exist_ok=True, parents=True)
        tokenizer.save_pretrained(str(PRETRAINED_MODEL_DIR))
        model.save_pretrained(str(PRETRAINED_MODEL_DIR))
        
        print(f"âœ… æ¨¡å‹å·²ä¸‹è½½åˆ°: {PRETRAINED_MODEL_DIR}")
        return str(PRETRAINED_MODEL_DIR)
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return model_name

def download_dataset():
    """ä¸‹è½½æ•°æ®é›†åˆ°æœ¬åœ°"""
    print("ğŸ“¥ ä¸‹è½½ Rotten Tomatoes æ•°æ®é›† (~1MB)")
    
    os.environ['HF_DATASETS_CACHE'] = str(DATASETS_DIR)
    
    try:
        dataset = load_dataset("rotten_tomatoes", cache_dir=str(DATASETS_DIR))
        print(f"âœ… æ•°æ®é›†å·²ä¸‹è½½åˆ°: {DATASETS_DIR}")
        print(f"   è®­ç»ƒé›†: {len(dataset['train']):,} æ¡")
        print(f"   éªŒè¯é›†: {len(dataset['validation']):,} æ¡")
        print(f"   æµ‹è¯•é›†: {len(dataset['test']):,} æ¡")
        return dataset
    except Exception as e:
        print(f"âŒ æ•°æ®é›†ä¸‹è½½å¤±è´¥: {e}")
        return None

class SentimentTrainer:
    def __init__(self, model_path):
        self.device = get_device()
        
        print(f"ğŸ”§ åŠ è½½æ¨¡å‹: {model_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_path,
            num_labels=2,  # å¥½è¯„/å·®è¯„
            ignore_mismatched_sizes=True
        )
        self.model.to(self.device)
        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)
        
    def preprocess_data(self, dataset):
        """æ•°æ®é¢„å¤„ç†"""
        print("ğŸ”„ é¢„å¤„ç†æ•°æ®...")
        
        def tokenize_function(examples):
            return self.tokenizer(
                examples['text'], 
                truncation=True, 
                padding=False,
                max_length=512
            )
        
        tokenized_dataset = dataset.map(
            tokenize_function, 
            batched=True,
            remove_columns=['text']
        )
        return tokenized_dataset
    
    def compute_metrics(self, eval_pred):
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)
        return {'accuracy': accuracy_score(labels, predictions)}
    
    def train(self, tokenized_dataset):
        """è®­ç»ƒæ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        
        # Mac M2 ä¼˜åŒ–å‚æ•°
        training_args = TrainingArguments(
            output_dir=str(TRAINED_MODEL_DIR),
            num_train_epochs=2,
            per_device_train_batch_size=8,
            per_device_eval_batch_size=8,
            warmup_steps=300,
            weight_decay=0.01,
            logging_steps=50,
            eval_strategy="epoch",
            save_strategy="epoch",
            load_best_model_at_end=True,
            metric_for_best_model="accuracy",
            report_to=None,
            dataloader_pin_memory=False,  # Macä¼˜åŒ–
            fp16=False,  # MPSä¸æ”¯æŒfp16
            dataloader_num_workers=0,  # Macä¼˜åŒ–
        )
        
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=tokenized_dataset['train'],
            eval_dataset=tokenized_dataset['validation'],
            tokenizer=self.tokenizer,
            data_collator=self.data_collator,
            compute_metrics=self.compute_metrics,
        )
        
        print("â±ï¸  è®­ç»ƒä¸­...ï¼ˆMac M2 é¢„è®¡ 10-20 åˆ†é’Ÿï¼‰")
        trainer.train()
        
        trainer.save_model(str(TRAINED_MODEL_DIR))
        self.tokenizer.save_pretrained(str(TRAINED_MODEL_DIR))
        
        print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {TRAINED_MODEL_DIR}")
        return trainer
    
    def evaluate(self, trainer, tokenized_dataset):
        """è¯„ä¼°æ¨¡å‹"""
        print("ğŸ“Š è¯„ä¼°æ¨¡å‹...")
        
        test_results = trainer.evaluate(eval_dataset=tokenized_dataset['test'])
        print(f"âœ… æµ‹è¯•å‡†ç¡®ç‡: {test_results['eval_accuracy']:.4f}")
        
        predictions = trainer.predict(tokenized_dataset['test'])
        y_pred = np.argmax(predictions.predictions, axis=1)
        y_true = predictions.label_ids
        
        print("\nğŸ“ˆ åˆ†ç±»æŠ¥å‘Š:")
        target_names = ['å·®è¯„ ğŸ‘', 'å¥½è¯„ ğŸ‘']
        print(classification_report(y_true, y_pred, target_names=target_names))
        
        # ä¿å­˜æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred)
        
        # é…ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
        plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'DejaVu Sans', 'SimHei', 'Helvetica']
        plt.rcParams['axes.unicode_minus'] = False
        
        plt.figure(figsize=(10, 8))
        ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                        xticklabels=target_names, yticklabels=target_names,
                        cbar_kws={'shrink': 0.8}, annot_kws={'size': 16})
        plt.title('æ··æ·†çŸ©é˜µ - ç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†æ', fontsize=18, fontweight='bold', pad=20)
        plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=16, labelpad=10)
        plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=16, labelpad=10)
        ax.tick_params(axis='both', which='major', labelsize=14)
        plt.tight_layout()
        plt.savefig('confusion_matrix.png', dpi=400, bbox_inches='tight', facecolor='white', edgecolor='none')
        plt.show()
        print("ğŸ“Š æ··æ·†çŸ©é˜µå·²ä¿å­˜ä¸º confusion_matrix.png")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¬ ç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†ææ¨¡å‹è®­ç»ƒ")
    print("=" * 50)
    
    # 1. åˆ›å»ºç›®å½•
    setup_directories()
    
    # 2. ä¸‹è½½æ¨¡å‹
    model_path = download_and_save_model()
    
    # 3. ä¸‹è½½æ•°æ®é›†
    dataset = download_dataset()
    if dataset is None:
        return
    
    # 4. æ˜¾ç¤ºæ•°æ®ç¤ºä¾‹
    print("\nğŸ“ æ•°æ®ç¤ºä¾‹:")
    for i in range(3):
        example = dataset['train'][i]
        label = "å¥½è¯„ ğŸ‘" if example['label'] == 1 else "å·®è¯„ ğŸ‘"
        print(f"  {i+1}. {example['text'][:80]}...")
        print(f"     æ ‡ç­¾: {label}")
    
    # 5. è®­ç»ƒæ¨¡å‹
    trainer = SentimentTrainer(model_path)
    tokenized_dataset = trainer.preprocess_data(dataset)
    model_trainer = trainer.train(tokenized_dataset)
    
    # 6. è¯„ä¼°æ¨¡å‹
    trainer.evaluate(model_trainer, tokenized_dataset)
    
    print("\nğŸ‰ å®Œæˆï¼")
    print(f"ğŸ“ æ¨¡å‹æ–‡ä»¶: {TRAINED_MODEL_DIR}")
    print("ğŸ’¡ è¿è¡Œ python predict.py è¿›è¡Œé¢„æµ‹")

if __name__ == "__main__":
    main() 