#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€é…ç½®ç®¡ç†æ¨¡å—
é›†ä¸­ç®¡ç†é¡¹ç›®çš„æ‰€æœ‰è·¯å¾„é…ç½®ã€æ¨¡å‹é…ç½®å’Œå…¶ä»–è®¾ç½®
é¿å…é…ç½®åˆ†æ•£ï¼Œæé«˜ç»´æŠ¤æ€§
"""

import os
import yaml
from pathlib import Path
from typing import Dict, Any, Optional


class ProjectConfig:
    """é¡¹ç›®é…ç½®ç®¡ç†ç±»"""
    
    def __init__(self):
        # é¡¹ç›®æ ¹ç›®å½• - åŸºäºè¿™ä¸ªæ–‡ä»¶çš„ä½ç½®è®¡ç®—
        self.PROJECT_ROOT = Path(__file__).parent.parent
        
        # åŸºç¡€ç›®å½•ç»“æ„
        self._setup_base_paths()

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.create_directories()
        
        # é»˜è®¤é…ç½®
        self._setup_default_config()
    
    def _setup_base_paths(self):
        """è®¾ç½®åŸºç¡€è·¯å¾„"""
        # èµ„æºç›®å½• (æ–°ç»“æ„)
        self.RESOURCES_DIR = self.PROJECT_ROOT / "resources"
        self.DATASETS_DIR = self.RESOURCES_DIR / "datasets"
        self.PRETRAINED_MODELS_DIR = self.RESOURCES_DIR / "pretrained_models"
        self.TRAINED_MODELS_DIR = self.RESOURCES_DIR / "trained_models"
        
        # # å…¶ä»–ç›®å½•
        # self.UTILS_DIR = self.PROJECT_ROOT / "utils"
        # self.CORE_DIR = self.PROJECT_ROOT / "core"
        # self.TASKS_DIR = self.PROJECT_ROOT / "tasks"
        # self.EXAMPLES_DIR = self.PROJECT_ROOT / "examples"
        
        # # ç¼“å­˜å’Œä¸´æ—¶ç›®å½•
        # self.CACHE_DIR = self.PROJECT_ROOT / "cache"
        # self.RESULTS_DIR = self.PROJECT_ROOT / "results"
        # self.LOGS_DIR = self.PROJECT_ROOT / "logs"
    
    def _setup_default_config(self):
        """è®¾ç½®é»˜è®¤é…ç½®"""
        self.DEFAULT_CONFIG = {
            # æ•°æ®é…ç½®
            "data": {
                # "dataset_name": "rotten_tomatoes",  # é»˜è®¤æ•°æ®é›†
                "cache_dir": str(self.DATASETS_DIR),
                "batch_size": 32,
                
                # æ”¯æŒçš„æ•°æ®é›†é…ç½®
                "available_datasets": {
                    "rotten_tomatoes": {
                        "name": "rotten_tomatoes",
                        "description": "ç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†æ",
                        "task_type": "sentiment_analysis",
                        "num_classes": 2,
                        "labels": ["negative", "positive"]
                    },
                    "imdb": {
                        "name": "imdb", 
                        "description": "IMDBç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†æ",
                        "task_type": "sentiment_analysis",
                        "num_classes": 2,
                        "labels": ["negative", "positive"]
                    },
                    "ag_news": {
                        "name": "ag_news",
                        "description": "AG Newsæ–°é—»åˆ†ç±»",
                        "task_type": "text_classification", 
                        "num_classes": 4,
                        "labels": ["World", "Sports", "Business", "Technology"]
                    },
                    "sst2": {
                        "name": "sst2",
                        "description": "Stanfordæƒ…æ„Ÿæ ‘åº“",
                        "task_type": "sentiment_analysis",
                        "num_classes": 2,
                        "labels": ["negative", "positive"]
                    }
                }
            },
            
            # æ¨¡å‹é…ç½®
            "models": {
                "sentiment_model": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                "embedding_model": "sentence-transformers/all-mpnet-base-v2",
                "models_cache_dir": str(self.PRETRAINED_MODELS_DIR)
            },
            
            # è®­ç»ƒé…ç½®
            "training": {
                "output_dir": str(self.TRAINED_MODELS_DIR),
                "cache_dir": str(self.PROJECT_ROOT / "cache"),
                "results_dir": str(self.PROJECT_ROOT / "results"),
                "logs_dir": str(self.PROJECT_ROOT / "logs")
            },
            
            # è®¾å¤‡é…ç½®
            "device": {
                "auto_select": True,
                "preferred": ["mps", "cuda", "cpu"]
            }
        }
    
    def get_path(self, path_name: str) -> Path:
        """
        è·å–æŒ‡å®šçš„è·¯å¾„
        
        Args:
            path_name: è·¯å¾„åç§°
            
        Returns:
            Path: è·¯å¾„å¯¹è±¡
        """
        if hasattr(self, path_name.upper()):
            return getattr(self, path_name.upper())
        else:
            raise ValueError(f"æœªçŸ¥çš„è·¯å¾„åç§°: {path_name}")
    
    def get_config(self, section: Optional[str] = None) -> Dict[str, Any]:
        """
        è·å–é…ç½®
        
        Args:
            section: é…ç½®èŠ‚åç§°ï¼Œä¸ºNoneæ—¶è¿”å›å…¨éƒ¨é…ç½®
            
        Returns:
            Dict: é…ç½®å­—å…¸
        """
        if section is None:
            return self.DEFAULT_CONFIG
        elif section in self.DEFAULT_CONFIG:
            return self.DEFAULT_CONFIG[section]
        else:
            raise ValueError(f"æœªçŸ¥çš„é…ç½®èŠ‚: {section}")
    
    def load_yaml_config(self, config_file: str) -> Dict[str, Any]:
        """
        åŠ è½½YAMLé…ç½®æ–‡ä»¶
        
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict: é…ç½®å­—å…¸
        """
        config_path = self.UTILS_DIR / config_file
        
        if not config_path.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def merge_config(self, yaml_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆå¹¶YAMLé…ç½®å’Œé»˜è®¤é…ç½®
        
        Args:
            yaml_config: YAMLé…ç½®å­—å…¸
            
        Returns:
            Dict: åˆå¹¶åçš„é…ç½®
        """
        merged_config = self.DEFAULT_CONFIG.copy()
        
        # æ·±åº¦åˆå¹¶é…ç½®
        def deep_merge(base_dict: Dict, update_dict: Dict):
            for key, value in update_dict.items():
                if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                    deep_merge(base_dict[key], value)
                else:
                    base_dict[key] = value
        
        deep_merge(merged_config, yaml_config)
        return merged_config
    
    def create_directories(self, directories: Optional[list] = None):
        """
        åˆ›å»ºå¿…è¦çš„ç›®å½•
        
        Args:
            directories: è¦åˆ›å»ºçš„ç›®å½•åˆ—è¡¨ï¼Œä¸ºNoneæ—¶åˆ›å»ºæ‰€æœ‰æ ‡å‡†ç›®å½•
        """
        if directories is None:
            directories = [
                self.RESOURCES_DIR,
                self.DATASETS_DIR,
                self.PRETRAINED_MODELS_DIR,
                self.TRAINED_MODELS_DIR,
                # æ³¨é‡Šæ‰çš„ç›®å½•ä¸å†è‡ªåŠ¨åˆ›å»º
                # self.CACHE_DIR,
                # self.RESULTS_DIR,
                # self.LOGS_DIR
            ]
        
        for directory in directories:
            directory.mkdir(exist_ok=True, parents=True)
        
        print(f"ğŸ“ ç›®å½•ç»“æ„å·²åˆ›å»º: {self.PROJECT_ROOT}")
    
    def get_dataset_info(self, dataset_name: str) -> Dict[str, Any]:
        """
        è·å–æŒ‡å®šæ•°æ®é›†çš„é…ç½®ä¿¡æ¯
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            
        Returns:
            Dict: æ•°æ®é›†é…ç½®ä¿¡æ¯
        """
        available_datasets = self.DEFAULT_CONFIG["data"]["available_datasets"]
        if dataset_name in available_datasets:
            return available_datasets[dataset_name]
        else:
            # è¿”å›é»˜è®¤é…ç½®
            return {
                "name": dataset_name,
                "description": f"æœªçŸ¥æ•°æ®é›†: {dataset_name}",
                "task_type": "text_classification",
                "num_classes": 2,
                "labels": ["label_0", "label_1"]
            }
    
    def list_available_datasets(self) -> list:
        """
        åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†
        
        Returns:
            list: æ•°æ®é›†åç§°åˆ—è¡¨
        """
        return list(self.DEFAULT_CONFIG["data"]["available_datasets"].keys())
    
    def get_device(self) -> str:
        """
        è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡
        
        Returns:
            str: è®¾å¤‡åç§°
        """
        try:
            import torch
            
            if torch.backends.mps.is_available():
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        except ImportError:
            return "cpu"
    
    def __str__(self) -> str:
        """è¿”å›é…ç½®æ¦‚è§ˆ"""
        return f"""
é¡¹ç›®é…ç½®æ¦‚è§ˆ:
  é¡¹ç›®æ ¹ç›®å½•: {self.PROJECT_ROOT}
  èµ„æºç›®å½•: {self.RESOURCES_DIR}
  æ•°æ®é›†ç›®å½•: {self.DATASETS_DIR}
  æ¨¡å‹ç›®å½•: {self.PRETRAINED_MODELS_DIR}
  è®­ç»ƒè¾“å‡ºç›®å½•: {self.TRAINED_MODELS_DIR}
  ç¼“å­˜ç›®å½•: {self.CACHE_DIR}
  ç»“æœç›®å½•: {self.RESULTS_DIR}
"""


# åˆ›å»ºå…¨å±€é…ç½®å®ä¾‹
config = ProjectConfig()

# å¯¼å‡ºå¸¸ç”¨è·¯å¾„å’Œé…ç½®ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
PROJECT_ROOT = config.PROJECT_ROOT
RESOURCES_DIR = config.RESOURCES_DIR
DATASETS_DIR = config.DATASETS_DIR
PRETRAINED_MODELS_DIR = config.PRETRAINED_MODELS_DIR
TRAINED_MODELS_DIR = config.TRAINED_MODELS_DIR

# å…¼å®¹æ—§è·¯å¾„åç§°
MODELS_DIR = config.PRETRAINED_MODELS_DIR  # å…¼å®¹æ€§åˆ«å
TRAINED_MODEL_DIR = config.TRAINED_MODELS_DIR  # å…¼å®¹æ€§åˆ«å

# å¯¼å‡ºé…ç½®è®¿é—®å‡½æ•°
def get_config(section: Optional[str] = None) -> Dict[str, Any]:
    """è·å–é…ç½®"""
    return config.get_config(section)

def load_task_config(config_file: str = "sentiment_analysis.yaml") -> Dict[str, Any]:
    """åŠ è½½ä»»åŠ¡é…ç½®"""
    yaml_config = config.load_yaml_config(config_file)
    return config.merge_config(yaml_config)

def setup_directories():
    """åˆ›å»ºé¡¹ç›®ç›®å½•"""
    config.create_directories()

def get_device() -> str:
    """è·å–æœ€ä½³è®¾å¤‡"""
    return config.get_device()

def get_dataset_info(dataset_name: str) -> Dict[str, Any]:
    """è·å–æ•°æ®é›†é…ç½®ä¿¡æ¯"""
    return config.get_dataset_info(dataset_name)

def list_available_datasets() -> list:
    """åˆ—å‡ºå¯ç”¨æ•°æ®é›†"""
    return config.list_available_datasets() 