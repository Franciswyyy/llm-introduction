#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é›†ç®¡ç†æ¨¡å—
è´Ÿè´£ä¸‹è½½ã€ç¼“å­˜å’ŒåŠ è½½Rotten Tomatoesæ•°æ®é›†
æ”¯æŒæœ¬åœ°ç¼“å­˜ï¼Œé¿å…é‡å¤ä¸‹è½½
"""

import os
from pathlib import Path
from datasets import load_dataset
import warnings
warnings.filterwarnings('ignore')

# é¡¹ç›®æœ¬åœ°è·¯å¾„é…ç½® - æŒ‡å‘é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent  # ä»utilsç›®å½•å›åˆ°é¡¹ç›®æ ¹ç›®å½•
MODELS_DIR = PROJECT_ROOT / "models"
DATASETS_DIR = PROJECT_ROOT / "datasets"
PRETRAINED_MODEL_DIR = MODELS_DIR / "twitter-roberta-base-sentiment-latest"
TRAINED_MODEL_DIR = PROJECT_ROOT / "trained_model"

def setup_directories():
    """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
    directories = [MODELS_DIR, DATASETS_DIR, TRAINED_MODEL_DIR]
    for directory in directories:
        directory.mkdir(exist_ok=True, parents=True)
    print(f"ğŸ“ ç›®å½•ç»“æ„å·²åˆ›å»º: {PROJECT_ROOT}")

def check_dataset_exists():
    """æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²å­˜åœ¨äºæœ¬åœ°"""
    # æ£€æŸ¥datasetsç¼“å­˜ç›®å½•
    cache_dir = DATASETS_DIR / "rotten_tomatoes"
    
    # Hugging Face datasetsçš„å…¸å‹ç¼“å­˜ç»“æ„
    if cache_dir.exists() and any(cache_dir.iterdir()):
        print("âœ… å‘ç°æœ¬åœ°æ•°æ®é›†ç¼“å­˜")
        return True
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡æŒ‡å®šçš„ç¼“å­˜ä½ç½®
    hf_cache = os.environ.get('HF_DATASETS_CACHE', '')
    if hf_cache and Path(hf_cache).exists():
        cache_path = Path(hf_cache) / "rotten_tomatoes"
        if cache_path.exists() and any(cache_path.iterdir()):
            print("âœ… å‘ç°HFç¼“å­˜ä¸­çš„æ•°æ®é›†")
            return True
    
    print("âŒ æœªå‘ç°æœ¬åœ°æ•°æ®é›†ç¼“å­˜")
    return False

def download_dataset_if_needed():
    """æ™ºèƒ½ä¸‹è½½ï¼šå¦‚æœæœ¬åœ°æ²¡æœ‰æ•°æ®é›†åˆ™ä¸‹è½½ï¼Œå¦åˆ™ä»æœ¬åœ°åŠ è½½"""
    print("ğŸ” æ£€æŸ¥æ•°æ®é›†çŠ¶æ€...")
    
    # åˆ›å»ºå¿…è¦ç›®å½•
    setup_directories()
    
    # è®¾ç½®ç¼“å­˜ç›®å½•
    os.environ['HF_DATASETS_CACHE'] = str(DATASETS_DIR)
    
    try:
        # å°è¯•åŠ è½½æ•°æ®é›†ï¼ˆä¼šè‡ªåŠ¨æ£€æŸ¥ç¼“å­˜ï¼‰
        print("ğŸ“¥ åŠ è½½ Rotten Tomatoes æ•°æ®é›†...")
        
        if check_dataset_exists():
            print("ğŸš€ ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ•°æ®é›†")
        else:
            print("ğŸŒ é¦–æ¬¡ä¸‹è½½æ•°æ®é›†åˆ°æœ¬åœ° (~1MB)")
        
        dataset = load_dataset("rotten_tomatoes", cache_dir=str(DATASETS_DIR))
        
        print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ:")
        print(f"   ğŸ“ ç¼“å­˜ä½ç½®: {DATASETS_DIR}")
        print(f"   ğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"      è®­ç»ƒé›†: {len(dataset['train']):,} æ¡")
        print(f"      éªŒè¯é›†: {len(dataset['validation']):,} æ¡")
        print(f"      æµ‹è¯•é›†: {len(dataset['test']):,} æ¡")
        
        return dataset
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å°è¯•æ‰‹åŠ¨åˆ é™¤ç¼“å­˜é‡æ–°ä¸‹è½½")
        return None

def get_dataset():
    """è·å–æ•°æ®é›†çš„ä¸»è¦æ¥å£å‡½æ•°"""
    return download_dataset_if_needed()

def clean_cache():
    """æ¸…ç†æ•°æ®é›†ç¼“å­˜ï¼ˆç”¨äºå¼ºåˆ¶é‡æ–°ä¸‹è½½ï¼‰"""
    import shutil
    
    if DATASETS_DIR.exists():
        print(f"ğŸ—‘ï¸  æ¸…ç†ç¼“å­˜ç›®å½•: {DATASETS_DIR}")
        shutil.rmtree(DATASETS_DIR)
        print("âœ… ç¼“å­˜å·²æ¸…ç†ï¼Œä¸‹æ¬¡è°ƒç”¨å°†é‡æ–°ä¸‹è½½")
    else:
        print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°ç¼“å­˜ç›®å½•")

def get_dataset_info():
    """è·å–æ•°æ®é›†ä¿¡æ¯è€Œä¸åŠ è½½å…¨éƒ¨æ•°æ®"""
    try:
        from datasets import get_dataset_infos
        infos = get_dataset_infos("rotten_tomatoes")
        return infos
    except Exception as e:
        print(f"âš ï¸  è·å–æ•°æ®é›†ä¿¡æ¯å¤±è´¥: {e}")
        return None

# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŸæœ‰å‡½æ•°å
def download_dataset():
    """ä¸‹è½½æ•°æ®é›†åˆ°æœ¬åœ°ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰"""
    return download_dataset_if_needed()

if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å—
    print("ğŸ§ª æµ‹è¯•æ•°æ®é›†ç®¡ç†æ¨¡å—")
    dataset = get_dataset()
    if dataset:
        print("âœ… æ¨¡å—æµ‹è¯•æˆåŠŸ")
    else:
        print("âŒ æ¨¡å—æµ‹è¯•å¤±è´¥")