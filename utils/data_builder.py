#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é›†ç®¡ç†æ¨¡å—
è´Ÿè´£ä¸‹è½½ã€ç¼“å­˜å’ŒåŠ è½½Rotten Tomatoesæ•°æ®é›†
æ”¯æŒæœ¬åœ°ç¼“å­˜ï¼Œé¿å…é‡å¤ä¸‹è½½
"""

import os
from pathlib import Path
from datasets import load_dataset
import warnings
warnings.filterwarnings('ignore')

# é¡¹ç›®æœ¬åœ°è·¯å¾„é…ç½® - æŒ‡å‘é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent  # ä»utilsç›®å½•å›åˆ°é¡¹ç›®æ ¹ç›®å½•
MODELS_DIR = PROJECT_ROOT / "models"
DATASETS_DIR = PROJECT_ROOT / "datasets"
PRETRAINED_MODEL_DIR = MODELS_DIR / "twitter-roberta-base-sentiment-latest"
TRAINED_MODEL_DIR = PROJECT_ROOT / "trained_model"

def setup_directories():
    """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
    directories = [MODELS_DIR, DATASETS_DIR, TRAINED_MODEL_DIR]
    for directory in directories:
        directory.mkdir(exist_ok=True, parents=True)
    print(f"ğŸ“ ç›®å½•ç»“æ„å·²åˆ›å»º: {PROJECT_ROOT}")

def check_dataset_exists(dataset_name="rotten_tomatoes"):
    """æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²å­˜åœ¨äºæœ¬åœ°"""
    # ä»configè¯»å–ç¼“å­˜ç›®å½•
    from .config import config
    cache_base_dir = Path(config.get_config('data')['cache_dir'])
    cache_dir = cache_base_dir / dataset_name
    
    # Hugging Face datasetsçš„å…¸å‹ç¼“å­˜ç»“æ„
    if cache_dir.exists() and any(cache_dir.iterdir()):
        print(f"âœ… å‘ç°æœ¬åœ°æ•°æ®é›†ç¼“å­˜: {dataset_name}")
        return True
  
    print(f"âŒ æœªå‘ç°æœ¬åœ°æ•°æ®é›†ç¼“å­˜: {dataset_name}")
    return False

def download_dataset_if_needed(dataset_name):
    """æ™ºèƒ½ä¸‹è½½ï¼šå¦‚æœæœ¬åœ°æ²¡æœ‰æ•°æ®é›†åˆ™ä¸‹è½½ï¼Œå¦åˆ™ä»æœ¬åœ°åŠ è½½"""
    print(f"ğŸ” æ£€æŸ¥æ•°æ®é›†çŠ¶æ€: {dataset_name}")
    
    # ä»configè¯»å–ç¼“å­˜ç›®å½•
    from .config import config
    cache_dir = config.get_config('data')['cache_dir']
    cache_path = Path(cache_dir) / dataset_name
    
    
    try:
        # Hugging Face datasetsçš„å…¸å‹ç¼“å­˜ç»“æ„
        if cache_path.exists() and any(cache_path.iterdir()):
            print(f"åŠ è½½ç¼“å­˜æ•°æ®é›†: {dataset_name}")
        dataset = load_dataset(dataset_name, cache_dir=str(cache_path))
        print(f"æ•°æ®é›†åŠ è½½æˆåŠŸ,è·¯å¾„ä¸º: {cache_path}")

        return dataset
 
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ•°æ®é›†åç§°æ˜¯å¦æ­£ç¡®")
        return None

def get_dataset_by_name(dataset_name):
    """æ ¹æ®æ•°æ®é›†åç§°è·å–æ•°æ®é›†"""
    return download_dataset_if_needed(dataset_name)

def get_dataset():
    """è·å–æ•°æ®é›†çš„ä¸»è¦æ¥å£å‡½æ•°ï¼ˆå‘åå…¼å®¹ï¼‰"""
    return download_dataset_if_needed("rotten_tomatoes")

def clean_cache():
    """æ¸…ç†æ•°æ®é›†ç¼“å­˜ï¼ˆç”¨äºå¼ºåˆ¶é‡æ–°ä¸‹è½½ï¼‰"""
    import shutil
    
    if DATASETS_DIR.exists():
        print(f"ğŸ—‘ï¸  æ¸…ç†ç¼“å­˜ç›®å½•: {DATASETS_DIR}")
        shutil.rmtree(DATASETS_DIR)
        print("âœ… ç¼“å­˜å·²æ¸…ç†ï¼Œä¸‹æ¬¡è°ƒç”¨å°†é‡æ–°ä¸‹è½½")
    else:
        print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°ç¼“å­˜ç›®å½•")

def get_dataset_info():
    """è·å–æ•°æ®é›†ä¿¡æ¯è€Œä¸åŠ è½½å…¨éƒ¨æ•°æ®"""
    try:
        from datasets import get_dataset_infos
        infos = get_dataset_infos("rotten_tomatoes")
        return infos
    except Exception as e:
        print(f"âš ï¸  è·å–æ•°æ®é›†ä¿¡æ¯å¤±è´¥: {e}")
        return None

# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŸæœ‰å‡½æ•°å
def download_dataset():
    """ä¸‹è½½æ•°æ®é›†åˆ°æœ¬åœ°ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰"""
    return download_dataset_if_needed()

if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å—
    print("ğŸ§ª æµ‹è¯•æ•°æ®é›†ç®¡ç†æ¨¡å—")
    dataset = get_dataset()
    if dataset:
        print("âœ… æ¨¡å—æµ‹è¯•æˆåŠŸ")
    else:
        print("âŒ æ¨¡å—æµ‹è¯•å¤±è´¥")