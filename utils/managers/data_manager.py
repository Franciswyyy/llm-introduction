#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®ç®¡ç†å™¨ - ä¸“æ³¨æ•°æ®é›†ç›¸å…³æ“ä½œ
éµå¾ªå•ä¸€èŒè´£åŸåˆ™ï¼Œåªå¤„ç†æ•°æ®é›†çš„ä¸‹è½½ã€ç¼“å­˜å’ŒåŠ è½½
"""

import sys
from pathlib import Path
from typing import Dict, Any, Optional
from datasets import load_dataset
import warnings
warnings.filterwarnings('ignore')

# é¡¹ç›®è·¯å¾„é…ç½®
PROJECT_ROOT = Path(__file__).parent.parent.parent
RESOURCES_DIR = PROJECT_ROOT / "resources"
DATASETS_DIR = RESOURCES_DIR / "datasets"

class DataManager:
    """
    æ•°æ®ç®¡ç†å™¨ç±»
    
    èŒè´£:
    - æ•°æ®é›†ä¸‹è½½å’Œç¼“å­˜
    - æ•°æ®é›†åŠ è½½å’ŒéªŒè¯
    - æ•°æ®é›†ä¿¡æ¯æŸ¥è¯¢
    """
    
    def __init__(self):
        self.cache_dir = DATASETS_DIR
        self._setup_directories()
    
    def _setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        self.cache_dir.mkdir(exist_ok=True, parents=True)
    
    def check_dataset_exists(self, dataset_name: str = "rotten_tomatoes") -> bool:
        """
        æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²ç¼“å­˜
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            
        Returns:
            bool: æ•°æ®é›†æ˜¯å¦å­˜åœ¨
        """
        dataset_path = self.cache_dir / dataset_name
        return dataset_path.exists() and any(dataset_path.iterdir())
    
    def download_dataset(self, dataset_name: str = "rotten_tomatoes") -> Dict[str, Any]:
        """
        ä¸‹è½½æ•°æ®é›†åˆ°æœ¬åœ°ç¼“å­˜
        
        Args:
            dataset_name: è¦ä¸‹è½½çš„æ•°æ®é›†åç§°
            
        Returns:
            Dict: åŠ è½½çš„æ•°æ®é›†
        """
        print(f"ğŸ”„ æ­£åœ¨ä¸‹è½½æ•°æ®é›†: {dataset_name}")
        print(f"ğŸ“ ç¼“å­˜ä½ç½®: {self.cache_dir}")
        
        try:
            dataset = load_dataset(dataset_name, cache_dir=str(self.cache_dir))
            print(f"âœ… æ•°æ®é›†ä¸‹è½½æˆåŠŸ")
            return dataset
        except Exception as e:
            print(f"âŒ æ•°æ®é›†ä¸‹è½½å¤±è´¥: {e}")
            raise
    
    def load_rotten_tomatoes(self) -> Dict[str, Any]:
        """
        åŠ è½½Rotten Tomatoesæ•°æ®é›†
        
        Returns:
            Dict: åŒ…å«train/validation/testçš„æ•°æ®é›†
        """
        dataset_name = "rotten_tomatoes"
        print(f"ğŸ” æ£€æŸ¥æ•°æ®é›†çŠ¶æ€: {dataset_name}")
        
        if self.check_dataset_exists(dataset_name):
            print("âœ… å‘ç°æœ¬åœ°æ•°æ®é›†ç¼“å­˜")
            print("ğŸš€ ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ•°æ®é›†")
        else:
            print("âŒ æœªå‘ç°æœ¬åœ°æ•°æ®é›†ç¼“å­˜")
            print("ğŸŒ é¦–æ¬¡ä¸‹è½½æ•°æ®é›†åˆ°æœ¬åœ°")
        
        # åŠ è½½æ•°æ®é›†ï¼ˆè‡ªåŠ¨å¤„ç†ç¼“å­˜ï¼‰
        dataset = load_dataset(dataset_name, cache_dir=str(self.cache_dir))
        
        # æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
        self._show_dataset_info(dataset)
        
        return dataset
    
    def load_custom_dataset(self, dataset_name: str) -> Dict[str, Any]:
        """
        åŠ è½½è‡ªå®šä¹‰æ•°æ®é›†
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            
        Returns:
            Dict: åŠ è½½çš„æ•°æ®é›†
        """
        if self.check_dataset_exists(dataset_name):
            print(f"âœ… ä»ç¼“å­˜åŠ è½½: {dataset_name}")
            return load_dataset(dataset_name, cache_dir=str(self.cache_dir))
        else:
            print(f"ğŸ“¥ ä¸‹è½½æ•°æ®é›†: {dataset_name}")
            return self.download_dataset(dataset_name)
    
    def get_dataset_info(self, dataset_name: str = "rotten_tomatoes") -> Dict[str, Any]:
        """
        è·å–æ•°æ®é›†ä¿¡æ¯
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            
        Returns:
            Dict: æ•°æ®é›†ä¿¡æ¯
        """
        if not self.check_dataset_exists(dataset_name):
            return {"exists": False, "message": f"æ•°æ®é›† {dataset_name} ä¸å­˜åœ¨"}
        
        try:
            dataset = load_dataset(dataset_name, cache_dir=str(self.cache_dir))
            info = {
                "exists": True,
                "splits": list(dataset.keys()),
                "features": dict(dataset[list(dataset.keys())[0]].features),
                "size": {split: len(data) for split, data in dataset.items()}
            }
            return info
        except Exception as e:
            return {"exists": False, "error": str(e)}
    
    def _show_dataset_info(self, dataset: Dict[str, Any]):
        """æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯"""
        print("âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ:")
        print(f"   ğŸ“ ç¼“å­˜ä½ç½®: {self.cache_dir}")
        print("   ğŸ“Š æ•°æ®ç»Ÿè®¡:")
        
        for split_name, split_data in dataset.items():
            split_size = len(split_data)
            print(f"      {split_name}: {split_size:,} æ¡")
    
    def clean_datasets_cache(self, dataset_name: Optional[str] = None):
        """
        æ¸…ç†æ•°æ®é›†ç¼“å­˜
        
        Args:
            dataset_name: è¦æ¸…ç†çš„æ•°æ®é›†åç§°ï¼ŒNoneè¡¨ç¤ºæ¸…ç†æ‰€æœ‰
        """
        import shutil
        
        if dataset_name:
            dataset_path = self.cache_dir / dataset_name
            if dataset_path.exists():
                shutil.rmtree(dataset_path)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ•°æ®é›†ç¼“å­˜: {dataset_name}")
            else:
                print(f"âŒ æ•°æ®é›†ç¼“å­˜ä¸å­˜åœ¨: {dataset_name}")
        else:
            if self.cache_dir.exists():
                shutil.rmtree(self.cache_dir)
                self._setup_directories()
                print("ğŸ—‘ï¸ å·²æ¸…ç†æ‰€æœ‰æ•°æ®é›†ç¼“å­˜")
            else:
                print("âŒ æ²¡æœ‰æ•°æ®é›†ç¼“å­˜éœ€è¦æ¸…ç†")
    
    def list_cached_datasets(self) -> list:
        """
        åˆ—å‡ºæ‰€æœ‰å·²ç¼“å­˜çš„æ•°æ®é›†
        
        Returns:
            list: ç¼“å­˜çš„æ•°æ®é›†åˆ—è¡¨
        """
        if not self.cache_dir.exists():
            return []
        
        cached_datasets = []
        for item in self.cache_dir.iterdir():
            if item.is_dir():
                cached_datasets.append(item.name)
        
        return cached_datasets 