#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹ç®¡ç†å™¨ - ä¸“æ³¨æ¨¡å‹ç›¸å…³æ“ä½œ
éµå¾ªå•ä¸€èŒè´£åŸåˆ™ï¼Œåªå¤„ç†æ¨¡å‹çš„ä¸‹è½½ã€ç¼“å­˜å’ŒåŠ è½½
"""

from pathlib import Path
from typing import Dict, Any, Optional, Union
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, pipeline
import warnings
warnings.filterwarnings('ignore')

# é¡¹ç›®è·¯å¾„é…ç½®
PROJECT_ROOT = Path(__file__).parent.parent.parent
RESOURCES_DIR = PROJECT_ROOT / "resources"
PRETRAINED_MODELS_DIR = RESOURCES_DIR / "pretrained_models"

class ModelManager:
    """
    æ¨¡å‹ç®¡ç†å™¨ç±»
    
    èŒè´£:
    - æ¨¡å‹ä¸‹è½½å’Œç¼“å­˜
    - æ¨¡å‹åŠ è½½å’Œåˆå§‹åŒ–
    - æ¨¡å‹é…ç½®ç®¡ç†
    """
    
    def __init__(self):
        self.models_dir = PRETRAINED_MODELS_DIR
        self._setup_directories()
        self._loaded_models = {}  # æ¨¡å‹ç¼“å­˜
    
    def _setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        self.models_dir.mkdir(exist_ok=True, parents=True)
    
    def _get_safe_model_name(self, model_name: str) -> str:
        """å°†æ¨¡å‹åè½¬æ¢ä¸ºå®‰å…¨çš„æ–‡ä»¶å"""
        return model_name.replace('/', '_').replace('\\', '_')
    
    def check_model_exists(self, model_name: str) -> bool:
        """
        æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç¼“å­˜
        
        Args:
            model_name: æ¨¡å‹åç§°
            
        Returns:
            bool: æ¨¡å‹æ˜¯å¦å­˜åœ¨
        """
        safe_name = self._get_safe_model_name(model_name)
        model_path = self.models_dir / safe_name
        config_file = model_path / "config.json"
        return config_file.exists()
    
    def download_model(self, model_name: str, model_type: str = "auto") -> str:
        """
        ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç¼“å­˜
        
        Args:
            model_name: æ¨¡å‹åç§°
            model_type: æ¨¡å‹ç±»å‹ ("auto", "classification", "embedding")
            
        Returns:
            str: æœ¬åœ°æ¨¡å‹è·¯å¾„
        """
        safe_name = self._get_safe_model_name(model_name)
        local_path = self.models_dir / safe_name
        
        print(f"ğŸ”„ æ­£åœ¨ä¸‹è½½æ¨¡å‹: {model_name}")
        print(f"ğŸ“ ä¿å­˜ä½ç½®: {local_path}")
        
        try:
            # ä¸‹è½½tokenizer
            print("   ğŸ“¥ ä¸‹è½½åˆ†è¯å™¨...")
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            tokenizer.save_pretrained(local_path)
            
            # ä¸‹è½½æ¨¡å‹
            print("   ğŸ“¥ ä¸‹è½½æ¨¡å‹...")
            if model_type == "classification":
                model = AutoModelForSequenceClassification.from_pretrained(model_name)
            else:
                model = AutoModel.from_pretrained(model_name)
            
            model.save_pretrained(local_path)
            print(f"âœ… æ¨¡å‹ä¸‹è½½æˆåŠŸ: {local_path}")
            return str(local_path)
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            # æ¸…ç†ä¸å®Œæ•´çš„ä¸‹è½½
            if local_path.exists():
                import shutil
                shutil.rmtree(local_path)
            raise
    
    def get_model_path(self, model_name: str, model_type: str = "auto") -> str:
        """
        è·å–æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä¸‹è½½
        
        Args:
            model_name: æ¨¡å‹åç§°
            model_type: æ¨¡å‹ç±»å‹
            
        Returns:
            str: æ¨¡å‹è·¯å¾„
        """
        print(f"ğŸ” æ£€æŸ¥æ¨¡å‹çŠ¶æ€: {model_name}")
        
        if self.check_model_exists(model_name):
            safe_name = self._get_safe_model_name(model_name)
            local_path = self.models_dir / safe_name
            print(f"âœ… å‘ç°æœ¬åœ°æ¨¡å‹ç¼“å­˜: {local_path}")
            return str(local_path)
        else:
            print("âŒ æœªå‘ç°æœ¬åœ°æ¨¡å‹ç¼“å­˜")
            print("ğŸŒ é¦–æ¬¡ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°")
            return self.download_model(model_name, model_type)
    
    def get_sentiment_pipeline(self, use_local: bool = True, model_name: str = "cardiffnlp/twitter-roberta-base-sentiment-latest"):
        """
        è·å–æƒ…æ„Ÿåˆ†æPipeline
        
        Args:
            use_local: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜
            model_name: æ¨¡å‹åç§°
            
        Returns:
            Pipeline: æƒ…æ„Ÿåˆ†æpipeline
        """
        if use_local:
            model_path = self.get_model_path(model_name, "classification")
        else:
            model_path = model_name
        
        print(f"ğŸ¤– åˆ›å»ºæƒ…æ„Ÿåˆ†æPipeline...")
        
        try:
            pipe = pipeline(
                "sentiment-analysis",
                model=model_path,
                tokenizer=model_path,
                return_all_scores=True,
                device=-1  # ä½¿ç”¨CPU
            )
            print("âœ… æƒ…æ„Ÿåˆ†ææ¨¡å‹åŠ è½½æˆåŠŸ")
            return pipe
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å¦‚æœæœ¬åœ°æ¨¡å‹å¤±è´¥ï¼Œå°è¯•ç½‘ç»œåŠ è½½
            if use_local and model_path != model_name:
                print("ğŸ”„ å°è¯•ä»ç½‘ç»œç›´æ¥åŠ è½½...")
                return self.get_sentiment_pipeline(use_local=False, model_name=model_name)
            raise
    
    def get_embedding_model(self, model_name: str = "sentence-transformers/all-mpnet-base-v2", use_local: bool = True):
        """
        è·å–åµŒå…¥æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            use_local: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜
            
        Returns:
            SentenceTransformer: åµŒå…¥æ¨¡å‹
        """
        # æ£€æŸ¥å†…å­˜ç¼“å­˜
        cache_key = f"embedding_{model_name}"
        if cache_key in self._loaded_models:
            print(f"ğŸš€ ä»å†…å­˜ç¼“å­˜åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}")
            return self._loaded_models[cache_key]
        
        if use_local:
            model_path = self.get_model_path(model_name, "embedding")
        else:
            model_path = model_name
        
        print(f"ğŸ¤– åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}")
        
        try:
            from sentence_transformers import SentenceTransformer
            model = SentenceTransformer(model_path)
            print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # ç¼“å­˜åˆ°å†…å­˜
            self._loaded_models[cache_key] = model
            return model
        except Exception as e:
            print(f"âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            if use_local and model_path != model_name:
                print("ğŸ”„ å°è¯•ä»ç½‘ç»œç›´æ¥åŠ è½½...")
                return self.get_embedding_model(model_name, use_local=False)
            raise
    
    def list_cached_models(self) -> list:
        """
        åˆ—å‡ºæ‰€æœ‰å·²ç¼“å­˜çš„æ¨¡å‹
        
        Returns:
            list: ç¼“å­˜çš„æ¨¡å‹ä¿¡æ¯åˆ—è¡¨
        """
        if not self.models_dir.exists():
            return []
        
        cached_models = []
        for model_dir in self.models_dir.iterdir():
            if model_dir.is_dir() and (model_dir / "config.json").exists():
                # å°†ç›®å½•åè½¬æ¢å›æ¨¡å‹å
                model_name = model_dir.name.replace('_', '/')
                
                # è®¡ç®—å¤§å°
                size = sum(f.stat().st_size for f in model_dir.rglob('*') if f.is_file())
                size_mb = size / (1024 * 1024)
                
                cached_models.append({
                    "name": model_name,
                    "size_mb": round(size_mb, 1),
                    "path": str(model_dir)
                })
        
        return cached_models
    
    def clear_model_cache(self, model_name: Optional[str] = None):
        """
        æ¸…ç†æ¨¡å‹ç¼“å­˜
        
        Args:
            model_name: è¦æ¸…ç†çš„æ¨¡å‹åç§°ï¼ŒNoneè¡¨ç¤ºæ¸…ç†æ‰€æœ‰
        """
        import shutil
        
        if model_name:
            safe_name = self._get_safe_model_name(model_name)
            model_path = self.models_dir / safe_name
            if model_path.exists():
                shutil.rmtree(model_path)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ¨¡å‹ç¼“å­˜: {model_name}")
                
                # æ¸…ç†å†…å­˜ç¼“å­˜
                cache_key = f"embedding_{model_name}"
                if cache_key in self._loaded_models:
                    del self._loaded_models[cache_key]
            else:
                print(f"âŒ æ¨¡å‹ç¼“å­˜ä¸å­˜åœ¨: {model_name}")
        else:
            if self.models_dir.exists():
                shutil.rmtree(self.models_dir)
                self._setup_directories()
                print("ğŸ—‘ï¸ å·²æ¸…ç†æ‰€æœ‰æ¨¡å‹ç¼“å­˜")
                
                # æ¸…ç†å†…å­˜ç¼“å­˜
                self._loaded_models.clear()
            else:
                print("âŒ æ²¡æœ‰æ¨¡å‹ç¼“å­˜éœ€è¦æ¸…ç†")
    
    def get_model_info(self, model_name: str) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Args:
            model_name: æ¨¡å‹åç§°
            
        Returns:
            Dict: æ¨¡å‹ä¿¡æ¯
        """
        info = {
            "name": model_name,
            "cached": self.check_model_exists(model_name),
            "in_memory": f"embedding_{model_name}" in self._loaded_models
        }
        
        if info["cached"]:
            safe_name = self._get_safe_model_name(model_name)
            model_path = self.models_dir / safe_name
            size = sum(f.stat().st_size for f in model_path.rglob('*') if f.is_file())
            info["size_mb"] = round(size / (1024 * 1024), 1)
            info["path"] = str(model_path)
        
        return info