#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹ç®¡ç†æ¨¡å—
è´Ÿè´£ä¸‹è½½ã€ç¼“å­˜å’ŒåŠ è½½å„ç§é¢„è®­ç»ƒæ¨¡å‹
æ”¯æŒæœ¬åœ°ç¼“å­˜ï¼Œé¿å…é‡å¤ä¸‹è½½
"""

import os
from pathlib import Path
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, pipeline
import warnings
warnings.filterwarnings('ignore')

# é¡¹ç›®æœ¬åœ°è·¯å¾„é…ç½® - ä»configè¯»å–
def get_models_cache_dir():
    """ä»configè·å–æ¨¡å‹ç¼“å­˜ç›®å½•"""
    from .config import config
    return Path(config.get_config('models')['models_cache_dir'])

def setup_model_directories():
    """åˆ›å»ºæ¨¡å‹ç›¸å…³ç›®å½•"""
    models_dir = get_models_cache_dir()
    models_dir.mkdir(exist_ok=True, parents=True)
    print(f"ğŸ“ æ¨¡å‹ç›®å½•ç»“æ„å·²åˆ›å»º: {models_dir}")

def check_model_exists(model_name: str) -> bool:
    """
    æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨äºæœ¬åœ°
    
    Args:
        model_name: æ¨¡å‹åç§°ï¼ˆå¦‚ 'cardiffnlp/twitter-roberta-base-sentiment-latest'ï¼‰
    
    Returns:
        bool: æ¨¡å‹æ˜¯å¦å­˜åœ¨
    """
    # å°†æ¨¡å‹åè½¬æ¢ä¸ºæœ¬åœ°ç›®å½•å
    safe_model_name = model_name.replace('/', '_')
    models_dir = get_models_cache_dir()
    model_path = models_dir / safe_model_name
    
    # æ£€æŸ¥æ˜¯å¦æœ‰config.jsonæ–‡ä»¶ï¼ˆè¡¨ç¤ºæ¨¡å‹å®Œæ•´ï¼‰
    config_file = model_path / "config.json"
    return config_file.exists()

def download_model(model_name: str, model_type: str = "auto") -> str:
    """
    ä¸‹è½½å¹¶ç¼“å­˜æ¨¡å‹åˆ°æœ¬åœ°
    
    Args:
        model_name: æ¨¡å‹åç§°
        model_type: æ¨¡å‹ç±»å‹ ('auto', 'classification', 'embedding')
    
    Returns:
        str: æœ¬åœ°æ¨¡å‹è·¯å¾„
    """
    setup_model_directories()
    safe_model_name = model_name.replace('/', '_')
    models_dir = get_models_cache_dir()
    local_model_path = models_dir / safe_model_name
    
    print(f"ğŸ”„ æ­£åœ¨ä¸‹è½½æ¨¡å‹: {model_name}")
    print(f"ğŸ“ ä¿å­˜ä½ç½®: {local_model_path}")
    
    try:
        # ä¸‹è½½tokenizer
        print("   ğŸ“¥ ä¸‹è½½åˆ†è¯å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        tokenizer.save_pretrained(local_model_path)
        
        # æ ¹æ®æ¨¡å‹ç±»å‹ä¸‹è½½å¯¹åº”çš„æ¨¡å‹
        print("   ğŸ“¥ ä¸‹è½½æ¨¡å‹...")
        if model_type == "classification":
            model = AutoModelForSequenceClassification.from_pretrained(model_name)
        else:
            model = AutoModel.from_pretrained(model_name)
        
        model.save_pretrained(local_model_path)
        
        print(f"âœ… æ¨¡å‹ä¸‹è½½æˆåŠŸ: {local_model_path}")
        return str(local_model_path)
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
        # å¦‚æœä¸‹è½½å¤±è´¥ï¼Œåˆ é™¤ä¸å®Œæ•´çš„æ–‡ä»¶
        if local_model_path.exists():
            import shutil
            shutil.rmtree(local_model_path)
        raise

def get_model_path(model_name: str, model_type: str = "auto") -> str:
    """
    è·å–æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä¸‹è½½
    
    Args:
        model_name: æ¨¡å‹åç§°
        model_type: æ¨¡å‹ç±»å‹
    
    Returns:
        str: æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°æˆ–è¿œç¨‹ï¼‰
    """
    print(f"ğŸ” æ£€æŸ¥æ¨¡å‹çŠ¶æ€: {model_name}")
    
    if check_model_exists(model_name):
        safe_model_name = model_name.replace('/', '_')
        models_dir = get_models_cache_dir()
        local_path = models_dir / safe_model_name
        print(f"âœ… å‘ç°æœ¬åœ°æ¨¡å‹ç¼“å­˜: {local_path}")
        return str(local_path)
    else:
        print("âŒ æœªå‘ç°æœ¬åœ°æ¨¡å‹ç¼“å­˜")
        print("ğŸŒ é¦–æ¬¡ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°")
        return download_model(model_name, model_type)

def get_sentiment_model(model_name: str = None, use_local: bool = True, device: str = "auto"):
    """
    è·å–æƒ…æ„Ÿåˆ†ææ¨¡å‹ (æ”¯æŒé…ç½®åŒ–)
    
    Args:
        model_name: æ¨¡å‹åç§°ï¼Œä¸ºNoneæ—¶ä»configè¯»å–
        use_local: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜
        device: è®¾å¤‡é€‰æ‹© ("auto", "cpu", "cuda", "mps")
    
    Returns:
        pipeline: æƒ…æ„Ÿåˆ†æpipeline
    """
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹åç§°ï¼Œä»configè¯»å–
    if model_name is None:
        from .config import config
        model_name = config.get_config('models')['sentiment_model']
    
    print(f"ğŸ¤– åŠ è½½æƒ…æ„Ÿåˆ†ææ¨¡å‹: {model_name}")
    
    if use_local:
        model_path = get_model_path(model_name, "classification")
    else:
        model_path = model_name
    
    # è‡ªåŠ¨è®¾å¤‡é€‰æ‹©
    if device == "auto":
        from .config import get_device
        device = get_device()
        print(f"ğŸ”§ è‡ªåŠ¨é€‰æ‹©è®¾å¤‡: {device}")
    
    # è½¬æ¢è®¾å¤‡æ ¼å¼
    if device == "cpu":
        device_id = -1
    elif device == "mps":
        device_id = "mps"
    elif device == "cuda":
        device_id = 0
    else:
        device_id = -1
    
    print(f"ğŸš€ åˆ›å»ºPipeline (è®¾å¤‡: {device})...")
    try:
        pipe = pipeline(
            "sentiment-analysis",
            model=model_path,
            tokenizer=model_path,
            return_all_scores=True,
            device=device_id
        )
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        return pipe
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        # å¦‚æœæœ¬åœ°æ¨¡å‹å¤±è´¥ï¼Œå°è¯•ç›´æ¥ä»ç½‘ç»œåŠ è½½
        if use_local and model_path != model_name:
            print("ğŸ”„ å°è¯•ä»ç½‘ç»œç›´æ¥åŠ è½½...")
            return get_sentiment_model(model_name, use_local=False, device=device)
        raise

def load_model_pipeline(model_name: str, use_local: bool = True, device: str = "auto"):
    """
    é€šç”¨çš„æ¨¡å‹pipelineåŠ è½½å‡½æ•° (ç±»ä¼¼HuggingFaceLoader.load_dataset)
    
    Args:
        model_name: æ¨¡å‹åç§°
        use_local: æ˜¯å¦ä½¿ç”¨æœ¬åœ°ç¼“å­˜
        device: è®¾å¤‡é€‰æ‹©
        
    Returns:
        pipeline: æ¨¡å‹pipeline
    """
    return get_sentiment_model(model_name, use_local, device)

def get_embedding_model(model_name: str = None, use_local: bool = True, device: str = "auto"):
    """
    è·å–åµŒå…¥æ¨¡å‹ (æ”¯æŒé…ç½®åŒ–)
    
    Args:
        model_name: åµŒå…¥æ¨¡å‹åç§°ï¼Œä¸ºNoneæ—¶ä»configè¯»å–
        use_local: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜
        device: è®¾å¤‡é€‰æ‹© ("auto", "cpu", "cuda", "mps")
    
    Returns:
        SentenceTransformer: åµŒå…¥æ¨¡å‹
    """
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹åç§°ï¼Œä»configè¯»å–
    if model_name is None:
        from .config import config
        model_name = config.get_config('models')['embedding_model']
    
    print(f"ğŸ¤– åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}")
    
    if use_local:
        model_path = get_model_path(model_name, "embedding")
    else:
        model_path = model_name
    
    # è‡ªåŠ¨è®¾å¤‡é€‰æ‹©
    if device == "auto":
        from .config import get_device
        device = get_device()
        print(f"ğŸ”§ è‡ªåŠ¨é€‰æ‹©è®¾å¤‡: {device}")
    
    print(f"ğŸš€ åˆ›å»ºSentenceTransformer (è®¾å¤‡: {device})...")
    try:
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer(model_path, device=device)
        print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
        return model
    except Exception as e:
        print(f"âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        # å¦‚æœæœ¬åœ°æ¨¡å‹å¤±è´¥ï¼Œå°è¯•ç›´æ¥ä»ç½‘ç»œåŠ è½½
        if use_local and model_path != model_name:
            print("ğŸ”„ å°è¯•ä»ç½‘ç»œç›´æ¥åŠ è½½...")
            return get_embedding_model(model_name, use_local=False, device=device)
        raise

def load_embedding_model(model_name: str, use_local: bool = True, device: str = "auto"):
    """
    ç®€æ´çš„åµŒå…¥æ¨¡å‹åŠ è½½å‡½æ•° (ç±»ä¼¼ load_model_pipeline)
    
    Args:
        model_name: æ¨¡å‹åç§°
        use_local: æ˜¯å¦ä½¿ç”¨æœ¬åœ°ç¼“å­˜
        device: è®¾å¤‡é€‰æ‹©
        
    Returns:
        SentenceTransformer: åµŒå…¥æ¨¡å‹
    """
    return get_embedding_model(model_name, use_local, device)

def list_cached_models():
    """åˆ—å‡ºæ‰€æœ‰å·²ç¼“å­˜çš„æ¨¡å‹"""
    setup_model_directories()
    models_dir = get_models_cache_dir()
    
    print("ğŸ“‹ å·²ç¼“å­˜çš„æ¨¡å‹:")
    if not models_dir.exists() or not list(models_dir.iterdir()):
        print("   (æ— )")
        return []
    
    cached_models = []
    for model_dir in models_dir.iterdir():
        if model_dir.is_dir() and (model_dir / "config.json").exists():
            # å°†ç›®å½•åè½¬æ¢å›æ¨¡å‹å
            model_name = model_dir.name.replace('_', '/')
            size = sum(f.stat().st_size for f in model_dir.rglob('*') if f.is_file())
            size_mb = size / (1024 * 1024)
            print(f"   ğŸ“¦ {model_name} ({size_mb:.1f}MB)")
            cached_models.append(model_name)
    
    return cached_models

def clear_model_cache(model_name: str = None):
    """
    æ¸…ç†æ¨¡å‹ç¼“å­˜
    
    Args:
        model_name: è¦æ¸…ç†çš„æ¨¡å‹åç§°ï¼ŒNoneè¡¨ç¤ºæ¸…ç†æ‰€æœ‰
    """
    import shutil
    
    if model_name:
        safe_model_name = model_name.replace('/', '_')
        model_path = PRETRAINED_MODELS_DIR / safe_model_name
        if model_path.exists():
            shutil.rmtree(model_path)
            print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ¨¡å‹ç¼“å­˜: {model_name}")
        else:
            print(f"âŒ æ¨¡å‹ç¼“å­˜ä¸å­˜åœ¨: {model_name}")
    else:
        if PRETRAINED_MODELS_DIR.exists():
            shutil.rmtree(PRETRAINED_MODELS_DIR)
            print("ğŸ—‘ï¸ å·²æ¸…ç†æ‰€æœ‰æ¨¡å‹ç¼“å­˜")
        else:
            print("âŒ æ²¡æœ‰æ¨¡å‹ç¼“å­˜éœ€è¦æ¸…ç†")

if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å‹ç®¡ç†åŠŸèƒ½
    print("ğŸ§ª æµ‹è¯•æ¨¡å‹ç®¡ç†å™¨")
    print("=" * 50)
    
    # åˆ—å‡ºå½“å‰ç¼“å­˜
    list_cached_models()
    
    # æµ‹è¯•è·å–æƒ…æ„Ÿåˆ†ææ¨¡å‹
    try:
        model = get_sentiment_model()
        print("âœ… æƒ…æ„Ÿåˆ†ææ¨¡å‹æµ‹è¯•æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æƒ…æ„Ÿåˆ†ææ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
    
    # å†æ¬¡åˆ—å‡ºç¼“å­˜
    print("\næ›´æ–°åçš„ç¼“å­˜:")
    list_cached_models() 