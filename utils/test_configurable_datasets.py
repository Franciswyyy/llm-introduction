#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•é…ç½®åŒ–æ•°æ®é›†åŠ è½½åŠŸèƒ½
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ä¸åŒçš„æ•°æ®é›†é…ç½®
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.append(str(PROJECT_ROOT))

from utils import get_dataset_by_name, load_task_config
from core.data.loaders import HuggingFaceLoader


def test_dataset_loading(dataset_name):
    """æµ‹è¯•åŠ è½½æŒ‡å®šæ•°æ®é›†"""
    print(f"\nğŸ§ª æµ‹è¯•åŠ è½½æ•°æ®é›†: {dataset_name}")
    print("=" * 50)
    
    try:
        # æ–¹æ³•1: ç›´æ¥ä½¿ç”¨å·¥å…·å‡½æ•°
        dataset = get_dataset_by_name(dataset_name)
        
        if dataset:
            print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®é›†: {dataset_name}")
            print(f"   æ•°æ®é›†åˆ†å‰²: {list(dataset.keys())}")
            for split_name in dataset.keys():
                print(f"   {split_name}: {len(dataset[split_name]):,} æ¡")
            
            # æ˜¾ç¤ºå‡ ä¸ªæ ·æœ¬
            if 'train' in dataset:
                print(f"\nğŸ“ æ ·æœ¬é¢„è§ˆ:")
                for i in range(min(2, len(dataset['train']))):
                    text = dataset['train'][i]['text']
                    label = dataset['train'][i]['label']
                    print(f"   æ–‡æœ¬: {text[:100]}...")
                    print(f"   æ ‡ç­¾: {label}")
        else:
            print(f"âŒ æ— æ³•åŠ è½½æ•°æ®é›†: {dataset_name}")
            
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")


def test_config_based_loading(config_file):
    """æµ‹è¯•åŸºäºé…ç½®æ–‡ä»¶çš„æ•°æ®åŠ è½½"""
    print(f"\nğŸ§ª æµ‹è¯•é…ç½®æ–‡ä»¶: {config_file}")
    print("=" * 50)
    
    try:
        # åŠ è½½é…ç½®
        config = load_task_config(config_file)
        data_config = config['data']
        
        print(f"ğŸ“Š é…ç½®ä¿¡æ¯:")
        print(f"   æ•°æ®é›†: {data_config.get('dataset_name')}")
        print(f"   åŠ è½½å™¨: {data_config.get('loader_type')}")
        
        # ä½¿ç”¨HuggingFaceLoader
        loader = HuggingFaceLoader(data_config)
        dataset = loader.load()
        
        if dataset:
            print(f"âœ… æˆåŠŸé€šè¿‡é…ç½®åŠ è½½æ•°æ®é›†")
            print(f"   æ ‡ç­¾åç§°: {loader.get_label_names()}")
            
            # æµ‹è¯•è·å–æ–‡æœ¬å’Œæ ‡ç­¾
            if 'train' in dataset:
                texts = loader.get_texts('train')
                labels = loader.get_labels('train')
                print(f"   è®­ç»ƒé›†æ–‡æœ¬æ•°: {len(texts)}")
                print(f"   è®­ç»ƒé›†æ ‡ç­¾æ•°: {len(labels)}")
        else:
            print(f"âŒ é…ç½®åŠ è½½å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")


def main():
    """ä¸»å‡½æ•° - æµ‹è¯•å¤šä¸ªæ•°æ®é›†"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•é…ç½®åŒ–æ•°æ®é›†åŠ è½½åŠŸèƒ½")
    
    # æµ‹è¯•ä¸åŒçš„æ•°æ®é›†
    datasets_to_test = [
        "rotten_tomatoes",  # é»˜è®¤æ•°æ®é›†
        "imdb",            # å¤§å‹æƒ…æ„Ÿåˆ†ææ•°æ®é›†
        # "ag_news",       # æ–°é—»åˆ†ç±»æ•°æ®é›† (å–æ¶ˆæ³¨é‡Šä»¥æµ‹è¯•)
    ]
    
    for dataset_name in datasets_to_test:
        test_dataset_loading(dataset_name)
    
    # æµ‹è¯•é…ç½®æ–‡ä»¶
    print(f"\nğŸ”§ æµ‹è¯•é…ç½®æ–‡ä»¶åŠ è½½")
    print("=" * 60)
    
    config_files = [
        "config.yaml",      # é»˜è®¤é…ç½®
        "imdb_config.yaml", # IMDBé…ç½®
        # "ag_news_config.yaml",  # AG Newsé…ç½® (å–æ¶ˆæ³¨é‡Šä»¥æµ‹è¯•)
    ]
    
    for config_file in config_files:
        config_path = Path(__file__).parent / config_file
        if config_path.exists():
            test_config_based_loading(config_file)
        else:
            print(f"âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
    
    print(f"\nâœ… æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main() 