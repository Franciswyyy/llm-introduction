# åµŒå…¥åˆ†ç±»ä»»åŠ¡æŒ‡å—

## ğŸ¯ æ¦‚è¿°
æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜äº†ä½¿ç”¨Sentence Transformerè¿›è¡Œæ–‡æœ¬åµŒå…¥åˆ†ç±»çš„å®Œæ•´æµç¨‹ï¼ŒåŒ…å«ä»£ç é€»è¾‘ã€è®¾è®¡æ€è·¯å’Œé€šç”¨æ¨¡æ¿ã€‚

---

## ğŸ“š ç›®å½•
1. [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
2. [é¡¹ç›®æ¶æ„](#é¡¹ç›®æ¶æ„)
3. [ä»£ç é€»è¾‘æµç¨‹](#ä»£ç é€»è¾‘æµç¨‹)
4. [ä¸¤ç§åˆ†ç±»æ–¹æ³•è¯¦è§£](#ä¸¤ç§åˆ†ç±»æ–¹æ³•è¯¦è§£)
5. [é€šç”¨ä»£ç æ¨¡æ¿](#é€šç”¨ä»£ç æ¨¡æ¿)
6. [æ‰©å±•å’Œä¼˜åŒ–](#æ‰©å±•å’Œä¼˜åŒ–)
7. [å¸¸è§é—®é¢˜è§£å†³](#å¸¸è§é—®é¢˜è§£å†³)

---

## æ ¸å¿ƒæ¦‚å¿µ

### ä»€ä¹ˆæ˜¯åµŒå…¥åˆ†ç±»ï¼Ÿ
**åµŒå…¥åˆ†ç±»**æ˜¯ä¸€ç§ç°ä»£NLPæ–¹æ³•ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´å‘é‡è¡¨ç¤ºï¼ˆåµŒå…¥ï¼‰ï¼Œç„¶ååœ¨è¿™äº›å‘é‡ä¸Šè¿›è¡Œåˆ†ç±»ã€‚

### å·¥ä½œåŸç†
```
åŸå§‹æ–‡æœ¬ â†’ Sentence Transformer â†’ åµŒå…¥å‘é‡ â†’ åˆ†ç±»å™¨ â†’ é¢„æµ‹ç»“æœ
```

### ä¼˜åŠ¿
- **è¯­ä¹‰ç†è§£**ï¼šæ•è·æ–‡æœ¬çš„æ·±å±‚è¯­ä¹‰ä¿¡æ¯
- **é¢„è®­ç»ƒä¼˜åŠ¿**ï¼šåˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†
- **é€šç”¨æ€§å¼º**ï¼šåŒä¸€å¥—æµç¨‹é€‚ç”¨äºå¤šç§æ–‡æœ¬åˆ†ç±»ä»»åŠ¡
- **æ•ˆæœä¼˜ç§€**ï¼šé€šå¸¸æ¯”ä¼ ç»Ÿæ–¹æ³•æ•ˆæœæ›´å¥½

---

## é¡¹ç›®æ¶æ„

### æ–‡ä»¶ç»„ç»‡ç»“æ„
```
llm-introduction/
â”œâ”€â”€ utils/                          # å·¥å…·æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py                 # æ¨¡å—åˆå§‹åŒ–
â”‚   â””â”€â”€ data_builder.py             # æ•°æ®ç®¡ç†å·¥å…·
â”œâ”€â”€ classification task/            # åˆ†ç±»ä»»åŠ¡ç›®å½•
â”‚   â”œâ”€â”€ embedding_classification.py # ä¸»åˆ†ç±»è„šæœ¬
â”‚   â”œâ”€â”€ requirements.txt           # ä¾èµ–åŒ…åˆ—è¡¨
â”‚   â””â”€â”€ README.md                  # ä»»åŠ¡è¯´æ˜æ–‡æ¡£
â””â”€â”€ .gitignore                     # å¿½ç•¥å¤§æ–‡ä»¶
```

### æ ¸å¿ƒä¾èµ–åŒ…
```
sentence-transformers  # å¥å­åµŒå…¥æ¨¡å‹
scikit-learn          # æœºå™¨å­¦ä¹ å·¥å…·
pandas               # æ•°æ®å¤„ç†
numpy                # æ•°å€¼è®¡ç®—
matplotlib           # å¯è§†åŒ–
seaborn              # ç»Ÿè®¡å¯è§†åŒ–
datasets             # Hugging Faceæ•°æ®é›†
torch                # PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
```

---

## ä»£ç é€»è¾‘æµç¨‹

### 1. æ•´ä½“æµç¨‹å›¾
```mermaid
graph TD
    A[åŠ è½½æ•°æ®é›†] --> B[åŠ è½½åµŒå…¥æ¨¡å‹]
    B --> C[ç”Ÿæˆæ–‡æœ¬åµŒå…¥]
    C --> D[æ–¹æ³•ä¸€: é€»è¾‘å›å½’åˆ†ç±»]
    C --> E[æ–¹æ³•äºŒ: ä½™å¼¦ç›¸ä¼¼åº¦åˆ†ç±»]
    D --> F[æ€§èƒ½è¯„ä¼°]
    E --> F
    F --> G[ç»“æœå¯è§†åŒ–]
```

### 2. è¯¦ç»†æ­¥éª¤åˆ†è§£

#### æ­¥éª¤1: æ•°æ®åŠ è½½æ¨¡å—
```python
def load_data():
    """
    åŠŸèƒ½ï¼šåŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›†
    è¾“å…¥ï¼šæ— ï¼ˆä»utilsæ¨¡å—è·å–ï¼‰
    è¾“å‡ºï¼šåŒ…å«train/validation/testçš„æ•°æ®é›†å­—å…¸
    """
    # è°ƒç”¨utils.get_dataset()è·å–Rotten Tomatoesæ•°æ®
    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    # è¿”å›ç»“æ„åŒ–æ•°æ®
```

#### æ­¥éª¤2: æ¨¡å‹åŠ è½½æ¨¡å—
```python
def load_embedding_model():
    """
    åŠŸèƒ½ï¼šåŠ è½½é¢„è®­ç»ƒçš„Sentence Transformeræ¨¡å‹
    è¾“å…¥ï¼šæ— 
    è¾“å‡ºï¼šSentenceTransformeræ¨¡å‹å®ä¾‹
    """
    # åŠ è½½'sentence-transformers/all-mpnet-base-v2'
    # è¯¥æ¨¡å‹è¾“å‡º768ç»´å‘é‡
    # æ”¯æŒå¤šè¯­è¨€ï¼Œæ•ˆæœä¼˜ç§€
```

#### æ­¥éª¤3: åµŒå…¥ç”Ÿæˆæ¨¡å—
```python
def generate_embeddings(model, data):
    """
    åŠŸèƒ½ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºåµŒå…¥å‘é‡
    è¾“å…¥ï¼šæ¨¡å‹å®ä¾‹ã€æ–‡æœ¬æ•°æ®
    è¾“å‡ºï¼šè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åµŒå…¥çŸ©é˜µ
    """
    # æ‰¹é‡å¤„ç†æ–‡æœ¬
    # æ˜¾ç¤ºè¿›åº¦æ¡
    # è¿”å›numpyæ•°ç»„æ ¼å¼çš„åµŒå…¥
```

#### æ­¥éª¤4A: é€»è¾‘å›å½’åˆ†ç±»æ¨¡å—
```python
def train_logistic_regression(train_embeddings, train_labels):
    """
    åŠŸèƒ½ï¼šè®­ç»ƒç›‘ç£å­¦ä¹ åˆ†ç±»å™¨
    è¾“å…¥ï¼šè®­ç»ƒé›†åµŒå…¥ã€è®­ç»ƒé›†æ ‡ç­¾
    è¾“å‡ºï¼šè®­ç»ƒå¥½çš„åˆ†ç±»å™¨
    """
    # ä½¿ç”¨sklearn.LogisticRegression
    # è®¾ç½®random_stateä¿è¯ç»“æœå¯é‡ç°
    # åœ¨åµŒå…¥å‘é‡ä¸Šè®­ç»ƒçº¿æ€§åˆ†ç±»å™¨
```

#### æ­¥éª¤4B: ä½™å¼¦ç›¸ä¼¼åº¦åˆ†ç±»æ¨¡å—
```python
def cosine_similarity_classification(train_embeddings, train_labels, test_embeddings):
    """
    åŠŸèƒ½ï¼šåŸºäºç›¸ä¼¼åº¦çš„æ— ç›‘ç£åˆ†ç±»
    è¾“å…¥ï¼šè®­ç»ƒé›†åµŒå…¥ã€æ ‡ç­¾ã€æµ‹è¯•é›†åµŒå…¥
    è¾“å‡ºï¼šé¢„æµ‹æ ‡ç­¾
    """
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¹³å‡åµŒå…¥å‘é‡
    # è®¡ç®—æµ‹è¯•æ ·æœ¬ä¸ç±»åˆ«ä¸­å¿ƒçš„ä½™å¼¦ç›¸ä¼¼åº¦
    # é€‰æ‹©ç›¸ä¼¼åº¦æœ€é«˜çš„ç±»åˆ«ä½œä¸ºé¢„æµ‹
```

#### æ­¥éª¤5: è¯„ä¼°æ¨¡å—
```python
def evaluate_performance(y_true, y_pred, method_name):
    """
    åŠŸèƒ½ï¼šå…¨é¢è¯„ä¼°åˆ†ç±»æ€§èƒ½
    è¾“å…¥ï¼šçœŸå®æ ‡ç­¾ã€é¢„æµ‹æ ‡ç­¾ã€æ–¹æ³•åç§°
    è¾“å‡ºï¼šè¯„ä¼°æŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨
    """
    # è®¡ç®—å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
    # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
    # åˆ›å»ºå’Œä¿å­˜æ··æ·†çŸ©é˜µå›¾åƒ
```

---

## ä¸¤ç§åˆ†ç±»æ–¹æ³•è¯¦è§£

### æ–¹æ³•ä¸€ï¼šé€»è¾‘å›å½’åˆ†ç±»

#### åŸç†
åœ¨åµŒå…¥å‘é‡ç©ºé—´ä¸­è®­ç»ƒçº¿æ€§åˆ†ç±»å™¨ï¼Œå­¦ä¹ æœ€ä¼˜çš„å†³ç­–è¾¹ç•Œã€‚

#### æ ¸å¿ƒä»£ç é€»è¾‘
```python
# 1. è®­ç»ƒé˜¶æ®µ
clf = LogisticRegression(random_state=42, max_iter=1000)
clf.fit(train_embeddings, train_labels)

# 2. é¢„æµ‹é˜¶æ®µ  
predictions = clf.predict(test_embeddings)

# 3. æ•°å­¦åŸç†
# å¯¹äºæ¯ä¸ªæµ‹è¯•æ ·æœ¬xï¼Œè®¡ç®—ï¼š
# prediction = argmax(wÂ·x + b)
# å…¶ä¸­wæ˜¯æƒé‡å‘é‡ï¼Œbæ˜¯åç½®
```

#### ä¼˜åŠ¿
- **ç›‘ç£å­¦ä¹ **ï¼šèƒ½å……åˆ†åˆ©ç”¨æ ‡ç­¾ä¿¡æ¯
- **å¿«é€Ÿè®­ç»ƒ**ï¼šçº¿æ€§æ¨¡å‹è®­ç»ƒé€Ÿåº¦å¿«
- **å¯è§£é‡Šæ€§**ï¼šå¯ä»¥åˆ†æç‰¹å¾æƒé‡
- **æ³›åŒ–èƒ½åŠ›**ï¼šåœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šè¡¨ç°ç¨³å®š

#### é€‚ç”¨åœºæ™¯
- æœ‰å……è¶³çš„æ ‡æ³¨æ•°æ®
- éœ€è¦å¿«é€Ÿè®­ç»ƒå’Œé¢„æµ‹
- è¦æ±‚æ¨¡å‹å¯è§£é‡Š

### æ–¹æ³•äºŒï¼šä½™å¼¦ç›¸ä¼¼åº¦åˆ†ç±»

#### åŸç†
è®¡ç®—æµ‹è¯•æ ·æœ¬ä¸å„ç±»åˆ«ä¸­å¿ƒç‚¹çš„ç›¸ä¼¼åº¦ï¼Œé€‰æ‹©æœ€ç›¸ä¼¼çš„ç±»åˆ«ã€‚

#### æ ¸å¿ƒä»£ç é€»è¾‘
```python
# 1. è®¡ç®—ç±»åˆ«ä¸­å¿ƒ
# å°†åµŒå…¥å’Œæ ‡ç­¾åˆå¹¶
df = pd.DataFrame(np.hstack([train_embeddings, labels.reshape(-1, 1)]))
# æŒ‰æ ‡ç­¾åˆ†ç»„ï¼Œè®¡ç®—æ¯ç»„çš„å¹³å‡åµŒå…¥
class_centers = df.groupby(embedding_dim).mean().iloc[:, :-1].values

# 2. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
similarity_matrix = cosine_similarity(test_embeddings, class_centers)

# 3. é€‰æ‹©æœ€ç›¸ä¼¼çš„ç±»åˆ«
predictions = np.argmax(similarity_matrix, axis=1)

# 4. æ•°å­¦åŸç†
# ä½™å¼¦ç›¸ä¼¼åº¦ = (AÂ·B) / (||A|| * ||B||)
# å€¼åŸŸä¸º[-1, 1]ï¼Œ1è¡¨ç¤ºå®Œå…¨ç›¸åŒ
```

#### ä¼˜åŠ¿
- **æ— éœ€è®­ç»ƒ**ï¼šç›´æ¥åŸºäºå‡ ä½•è·ç¦»è®¡ç®—
- **ç›´è§‚æ˜“æ‡‚**ï¼šåŸºäºç›¸ä¼¼åº¦çš„è‡ªç„¶æ€æƒ³
- **å°‘æ ·æœ¬å‹å¥½**ï¼šåœ¨æ ·æœ¬ä¸è¶³æ—¶ä»èƒ½å·¥ä½œ
- **è®¡ç®—é«˜æ•ˆ**ï¼šåªéœ€çŸ©é˜µè¿ç®—

#### é€‚ç”¨åœºæ™¯
- æ ‡æ³¨æ•°æ®è¾ƒå°‘
- éœ€è¦å¿«é€ŸåŸå‹éªŒè¯
- ç±»åˆ«å¯ä»¥ç”¨ä¸­å¿ƒç‚¹å¾ˆå¥½è¡¨ç¤º

---

## é€šç”¨ä»£ç æ¨¡æ¿

### åŸºç¡€æ¨¡æ¿ç»“æ„
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨åµŒå…¥åˆ†ç±»æ¨¡æ¿
é€‚ç”¨äºå„ç§æ–‡æœ¬åˆ†ç±»ä»»åŠ¡
"""

import sys
import numpy as np
import pandas as pd
from pathlib import Path
from sentence_transformers import SentenceTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns

class EmbeddingClassifier:
    """åµŒå…¥åˆ†ç±»å™¨é€šç”¨ç±»"""
    
    def __init__(self, model_name='sentence-transformers/all-mpnet-base-v2'):
        """åˆå§‹åŒ–åˆ†ç±»å™¨"""
        self.model_name = model_name
        self.model = None
        self.classifier = None
        self.class_centers = None
        
    def load_model(self):
        """åŠ è½½åµŒå…¥æ¨¡å‹"""
        print(f"ğŸ¤– æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name}")
        self.model = SentenceTransformer(self.model_name)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        return self
    
    def generate_embeddings(self, texts, show_progress=True):
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥"""
        if self.model is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨load_model()åŠ è½½æ¨¡å‹")
        return self.model.encode(texts, show_progress_bar=show_progress)
    
    def train_supervised(self, train_texts, train_labels, **kwargs):
        """è®­ç»ƒç›‘ç£åˆ†ç±»å™¨"""
        print("ğŸ¯ è®­ç»ƒç›‘ç£åˆ†ç±»å™¨...")
        train_embeddings = self.generate_embeddings(train_texts)
        
        self.classifier = LogisticRegression(random_state=42, **kwargs)
        self.classifier.fit(train_embeddings, train_labels)
        print("âœ… ç›‘ç£åˆ†ç±»å™¨è®­ç»ƒå®Œæˆ")
        return self
    
    def prepare_similarity_classifier(self, train_texts, train_labels):
        """å‡†å¤‡ç›¸ä¼¼åº¦åˆ†ç±»å™¨"""
        print("ğŸ”„ å‡†å¤‡ç›¸ä¼¼åº¦åˆ†ç±»å™¨...")
        train_embeddings = self.generate_embeddings(train_texts)
        
        # è®¡ç®—ç±»åˆ«ä¸­å¿ƒ
        df = pd.DataFrame(np.hstack([train_embeddings, 
                                   np.array(train_labels).reshape(-1, 1)]))
        self.class_centers = df.groupby(df.columns[-1]).mean().iloc[:, :-1].values
        print("âœ… ç›¸ä¼¼åº¦åˆ†ç±»å™¨å‡†å¤‡å®Œæˆ")
        return self
    
    def predict_supervised(self, test_texts):
        """ç›‘ç£æ–¹æ³•é¢„æµ‹"""
        if self.classifier is None:
            raise ValueError("è¯·å…ˆè®­ç»ƒç›‘ç£åˆ†ç±»å™¨")
        test_embeddings = self.generate_embeddings(test_texts)
        return self.classifier.predict(test_embeddings)
    
    def predict_similarity(self, test_texts):
        """ç›¸ä¼¼åº¦æ–¹æ³•é¢„æµ‹"""
        if self.class_centers is None:
            raise ValueError("è¯·å…ˆå‡†å¤‡ç›¸ä¼¼åº¦åˆ†ç±»å™¨")
        test_embeddings = self.generate_embeddings(test_texts)
        similarities = cosine_similarity(test_embeddings, self.class_centers)
        return np.argmax(similarities, axis=1)
    
    def evaluate(self, y_true, y_pred, method_name="", class_names=None):
        """è¯„ä¼°æ€§èƒ½"""
        print(f"\nğŸ“Š {method_name}æ€§èƒ½è¯„ä¼°:")
        print("=" * 50)
        
        report = classification_report(y_true, y_pred, 
                                     target_names=class_names, 
                                     digits=4)
        print(report)
        
        # æ··æ·†çŸ©é˜µå¯è§†åŒ–
        cm = confusion_matrix(y_true, y_pred)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=class_names or range(len(np.unique(y_true))),
                    yticklabels=class_names or range(len(np.unique(y_true))))
        plt.title(f'{method_name}æ··æ·†çŸ©é˜µ')
        plt.ylabel('å®é™…æ ‡ç­¾')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.show()

# ä½¿ç”¨ç¤ºä¾‹
def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    # 1. åˆ›å»ºåˆ†ç±»å™¨å®ä¾‹
    classifier = EmbeddingClassifier()
    
    # 2. åŠ è½½æ¨¡å‹
    classifier.load_model()
    
    # 3. å‡†å¤‡æ•°æ®ï¼ˆè¿™é‡Œéœ€è¦æ›¿æ¢ä¸ºå®é™…æ•°æ®ï¼‰
    train_texts = ["æ­£é¢æ–‡æœ¬ç¤ºä¾‹", "è´Ÿé¢æ–‡æœ¬ç¤ºä¾‹"]
    train_labels = [1, 0]
    test_texts = ["å¾…åˆ†ç±»æ–‡æœ¬"]
    test_labels = [1]
    
    # 4. è®­ç»ƒå’Œé¢„æµ‹ï¼ˆç›‘ç£æ–¹æ³•ï¼‰
    classifier.train_supervised(train_texts, train_labels)
    supervised_pred = classifier.predict_supervised(test_texts)
    
    # 5. å‡†å¤‡å’Œé¢„æµ‹ï¼ˆç›¸ä¼¼åº¦æ–¹æ³•ï¼‰
    classifier.prepare_similarity_classifier(train_texts, train_labels)
    similarity_pred = classifier.predict_similarity(test_texts)
    
    # 6. è¯„ä¼°ç»“æœ
    classifier.evaluate(test_labels, supervised_pred, "ç›‘ç£å­¦ä¹ ", ["è´Ÿé¢", "æ­£é¢"])
    classifier.evaluate(test_labels, similarity_pred, "ç›¸ä¼¼åº¦åŒ¹é…", ["è´Ÿé¢", "æ­£é¢"])

if __name__ == "__main__":
    example_usage()
```

---

## æ‰©å±•å’Œä¼˜åŒ–

### 1. æ¨¡å‹é€‰æ‹©ä¼˜åŒ–
```python
# ä¸åŒä»»åŠ¡çš„æ¨èæ¨¡å‹
RECOMMENDED_MODELS = {
    "ä¸­æ–‡æ–‡æœ¬": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "è‹±æ–‡æ–‡æœ¬": "sentence-transformers/all-mpnet-base-v2", 
    "çŸ­æ–‡æœ¬": "sentence-transformers/all-MiniLM-L6-v2",
    "é•¿æ–‡æœ¬": "sentence-transformers/all-mpnet-base-v2",
    "ä»£ç æ–‡æœ¬": "microsoft/codebert-base"
}
```

### 2. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
```python
# æ‰¹å¤„ç†ä¼˜åŒ–
def batch_encode(texts, model, batch_size=32):
    """æ‰¹é‡ç¼–ç ä»¥æé«˜æ•ˆç‡"""
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        batch_embeddings = model.encode(batch)
        embeddings.append(batch_embeddings)
    return np.vstack(embeddings)

# ç¼“å­˜æœºåˆ¶
import pickle
def save_embeddings(embeddings, filepath):
    """ä¿å­˜åµŒå…¥ä»¥é¿å…é‡å¤è®¡ç®—"""
    with open(filepath, 'wb') as f:
        pickle.dump(embeddings, f)

def load_embeddings(filepath):
    """åŠ è½½å·²ä¿å­˜çš„åµŒå…¥"""
    with open(filepath, 'rb') as f:
        return pickle.load(f)
```

### 3. é«˜çº§åˆ†ç±»æ–¹æ³•
```python
# æ”¯æŒå‘é‡æœº
from sklearn.svm import SVC

def train_svm_classifier(train_embeddings, train_labels):
    """è®­ç»ƒSVMåˆ†ç±»å™¨"""
    svm = SVC(kernel='rbf', random_state=42)
    svm.fit(train_embeddings, train_labels)
    return svm

# éšæœºæ£®æ—
from sklearn.ensemble import RandomForestClassifier

def train_rf_classifier(train_embeddings, train_labels):
    """è®­ç»ƒéšæœºæ£®æ—åˆ†ç±»å™¨"""
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(train_embeddings, train_labels)
    return rf

# ç¥ç»ç½‘ç»œåˆ†ç±»å™¨
from sklearn.neural_network import MLPClassifier

def train_nn_classifier(train_embeddings, train_labels):
    """è®­ç»ƒç¥ç»ç½‘ç»œåˆ†ç±»å™¨"""
    nn = MLPClassifier(hidden_layer_sizes=(128, 64), random_state=42)
    nn.fit(train_embeddings, train_labels)
    return nn
```

### 4. å¤šåˆ†ç±»ç­–ç•¥
```python
def handle_multiclass(train_embeddings, train_labels, strategy='ovr'):
    """å¤„ç†å¤šåˆ†ç±»é—®é¢˜"""
    from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier
    
    base_classifier = LogisticRegression(random_state=42)
    
    if strategy == 'ovr':
        # ä¸€å¯¹å…¶ä½™ç­–ç•¥
        classifier = OneVsRestClassifier(base_classifier)
    elif strategy == 'ovo':
        # ä¸€å¯¹ä¸€ç­–ç•¥  
        classifier = OneVsOneClassifier(base_classifier)
    else:
        # ç›´æ¥å¤šåˆ†ç±»
        classifier = base_classifier
    
    classifier.fit(train_embeddings, train_labels)
    return classifier
```

---

## å¸¸è§é—®é¢˜è§£å†³

### 1. å†…å­˜ä¸è¶³é—®é¢˜
```python
# è§£å†³æ–¹æ¡ˆï¼šåˆ†æ‰¹å¤„ç†
def encode_in_batches(texts, model, batch_size=16):
    """åˆ†æ‰¹ç¼–ç å¤§é‡æ–‡æœ¬"""
    embeddings = []
    for i in tqdm(range(0, len(texts), batch_size)):
        batch = texts[i:i+batch_size]
        batch_emb = model.encode(batch, show_progress_bar=False)
        embeddings.append(batch_emb)
        
        # é‡Šæ”¾å†…å­˜
        if i % 1000 == 0:
            gc.collect()
    
    return np.vstack(embeddings)
```

### 2. æ¨¡å‹ä¸‹è½½å¤±è´¥
```python
# è§£å†³æ–¹æ¡ˆï¼šç¦»çº¿æ¨¡å‹æˆ–é•œåƒæº
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# æˆ–ä½¿ç”¨æœ¬åœ°æ¨¡å‹
model = SentenceTransformer('/path/to/local/model')
```

### 3. åˆ†ç±»æ•ˆæœä¸ä½³
```python
# è§£å†³æ–¹æ¡ˆï¼šç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹è°ƒä¼˜
def feature_engineering(embeddings):
    """ç‰¹å¾å·¥ç¨‹å¢å¼º"""
    from sklearn.preprocessing import StandardScaler, PCA
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    embeddings_scaled = scaler.fit_transform(embeddings)
    
    # é™ç»´ï¼ˆå¯é€‰ï¼‰
    pca = PCA(n_components=0.95)  # ä¿ç•™95%æ–¹å·®
    embeddings_reduced = pca.fit_transform(embeddings_scaled)
    
    return embeddings_reduced, scaler, pca

# è¶…å‚æ•°è°ƒä¼˜
from sklearn.model_selection import GridSearchCV

def tune_hyperparameters(train_embeddings, train_labels):
    """è¶…å‚æ•°è°ƒä¼˜"""
    param_grid = {
        'C': [0.1, 1, 10, 100],
        'max_iter': [1000, 2000, 5000]
    }
    
    lr = LogisticRegression(random_state=42)
    grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='f1_macro')
    grid_search.fit(train_embeddings, train_labels)
    
    return grid_search.best_estimator_
```

### 4. ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
```python
# è§£å†³æ–¹æ¡ˆï¼šæ ·æœ¬æƒé‡å¹³è¡¡
def handle_class_imbalance(train_embeddings, train_labels):
    """å¤„ç†ç±»åˆ«ä¸å¹³è¡¡"""
    from sklearn.utils.class_weight import compute_class_weight
    
    # è®¡ç®—ç±»åˆ«æƒé‡
    classes = np.unique(train_labels)
    class_weights = compute_class_weight('balanced', 
                                       classes=classes, 
                                       y=train_labels)
    class_weight_dict = dict(zip(classes, class_weights))
    
    # ä½¿ç”¨æƒé‡è®­ç»ƒ
    classifier = LogisticRegression(class_weight=class_weight_dict, 
                                  random_state=42)
    classifier.fit(train_embeddings, train_labels)
    
    return classifier
```

---

## ğŸ¯ æœ€ä½³å®è·µæ€»ç»“

### 1. å¼€å‘æµç¨‹
1. **æ•°æ®æ¢ç´¢**ï¼šäº†è§£æ•°æ®åˆ†å¸ƒå’Œç‰¹ç‚¹
2. **åŸºçº¿å»ºç«‹**ï¼šå…ˆç”¨ç®€å•æ–¹æ³•å»ºç«‹åŸºçº¿
3. **æ¨¡å‹é€‰æ‹©**ï¼šæ ¹æ®ä»»åŠ¡ç‰¹ç‚¹é€‰æ‹©åˆé€‚çš„åµŒå…¥æ¨¡å‹
4. **æ–¹æ³•å¯¹æ¯”**ï¼šåŒæ—¶å°è¯•ç›‘ç£å’Œæ— ç›‘ç£æ–¹æ³•
5. **ç»“æœåˆ†æ**ï¼šæ·±å…¥åˆ†æé”™è¯¯æ¡ˆä¾‹
6. **è¿­ä»£ä¼˜åŒ–**ï¼šåŸºäºåˆ†æç»“æœæŒç»­æ”¹è¿›

### 2. ä»£ç ç»„ç»‡
- **æ¨¡å—åŒ–è®¾è®¡**ï¼šå°†ä¸åŒåŠŸèƒ½æ‹†åˆ†ä¸ºç‹¬ç«‹å‡½æ•°
- **é…ç½®ç®¡ç†**ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ç®¡ç†è¶…å‚æ•°
- **æ—¥å¿—è®°å½•**ï¼šè®°å½•è®­ç»ƒè¿‡ç¨‹å’Œç»“æœ
- **å¼‚å¸¸å¤„ç†**ï¼šä¼˜é›…å¤„ç†å„ç§å¼‚å¸¸æƒ…å†µ
- **æ–‡æ¡£å®Œå–„**ï¼šä¸ºæ‰€æœ‰å‡½æ•°æ·»åŠ è¯¦ç»†æ–‡æ¡£

### 3. æ€§èƒ½ç›‘æ§
- **äº¤å‰éªŒè¯**ï¼šä½¿ç”¨k-foldéªŒè¯æ¨¡å‹ç¨³å®šæ€§
- **å¤šæŒ‡æ ‡è¯„ä¼°**ï¼šä¸åªçœ‹å‡†ç¡®ç‡ï¼Œå…³æ³¨ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1
- **æ··æ·†çŸ©é˜µåˆ†æ**ï¼šè¯†åˆ«åˆ†ç±»é”™è¯¯æ¨¡å¼
- **å­¦ä¹ æ›²çº¿**ï¼šåˆ†ææ¨¡å‹æ˜¯å¦è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆ

è¿™ä¸ªæ¡†æ¶ä¸ºæ‚¨æä¾›äº†å®Œæ•´çš„åµŒå…¥åˆ†ç±»è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥æ ¹æ®å…·ä½“ä»»åŠ¡éœ€æ±‚è¿›è¡Œè°ƒæ•´å’Œæ‰©å±•ã€‚ 